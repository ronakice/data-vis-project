{
    "x": {
        "0": 11.011292457580566,
        "1": 6.156713008880615,
        "2": 14.240878105163574,
        "3": 18.71257209777832,
        "4": 19.185091018676758,
        "5": 17.757749557495117,
        "6": 3.7933189868927,
        "7": 17.346742630004883,
        "8": 11.858014106750488,
        "9": 15.456168174743652,
        "10": 8.779067039489746,
        "11": 9.13174819946289,
        "12": 7.352838516235352,
        "13": 17.24557113647461,
        "14": 2.3589653968811035,
        "15": 15.857989311218262,
        "16": 7.8030500411987305,
        "17": -1.678708791732788,
        "18": 8.688693046569824,
        "19": 4.290965557098389,
        "20": 19.610685348510742,
        "21": 13.563776969909668,
        "22": 13.913220405578613,
        "23": 3.582181692123413,
        "24": 14.052779197692871,
        "25": 19.560007095336914,
        "26": 8.657358169555664,
        "27": 4.8147196769714355,
        "28": 7.995233058929443,
        "29": 13.118759155273438,
        "30": 3.0833518505096436,
        "31": -7.803526401519775,
        "32": 14.687341690063477,
        "33": 5.0606207847595215,
        "34": 3.5931456089019775,
        "35": 1.6332155466079712,
        "36": -13.210665702819824,
        "37": 19.698183059692383,
        "38": 4.3770365715026855,
        "39": 19.96881103515625,
        "40": 7.076900005340576,
        "41": -13.982436180114746,
        "42": 20.66099739074707,
        "43": -1.1827661991119385,
        "44": 8.187230110168457,
        "45": 14.424200057983398,
        "46": 11.048433303833008,
        "47": 1.7895759344100952,
        "48": 7.167497158050537,
        "49": 2.6914174556732178,
        "50": 10.211276054382324,
        "51": 16.836130142211914,
        "52": 13.33615779876709,
        "53": 12.264178276062012,
        "54": 7.511374473571777,
        "55": 9.519103050231934,
        "56": -13.5646333694458,
        "57": 10.68822956085205,
        "58": 12.3649320602417,
        "59": 9.403850555419922,
        "60": 15.243465423583984,
        "61": 2.5699827671051025,
        "62": -2.989353895187378,
        "63": 14.7144775390625,
        "64": 0.6255497336387634,
        "65": 4.685359954833984,
        "66": 19.697057723999023,
        "67": 0.5259689092636108,
        "68": 4.818374156951904,
        "69": 13.324238777160645,
        "70": 4.865560531616211,
        "71": 2.64532732963562,
        "72": -2.236654043197632,
        "73": 18.58647918701172,
        "74": 8.73466968536377,
        "75": -0.24654048681259155,
        "76": 13.319783210754395,
        "77": -9.052374839782715,
        "78": 8.753474235534668,
        "79": 7.812932968139648,
        "80": 10.338377952575684,
        "81": 8.381004333496094,
        "82": 7.571786880493164,
        "83": 1.0322892665863037,
        "84": 2.3848373889923096,
        "85": 1.2016358375549316,
        "86": 10.323336601257324,
        "87": 13.011056900024414,
        "88": 4.852607250213623,
        "89": 16.29330062866211,
        "90": 16.027685165405273,
        "91": 7.333901882171631,
        "92": 12.45335578918457,
        "93": -4.752339839935303,
        "94": 5.831212043762207,
        "95": 5.673777103424072,
        "96": -10.332244873046875,
        "97": 13.332236289978027,
        "98": 4.998582363128662,
        "99": -3.5837814807891846,
        "100": 8.7658109664917,
        "101": 1.1933528184890747,
        "102": 4.074538230895996,
        "103": 16.628477096557617,
        "104": -12.707925796508789,
        "105": 6.906070232391357,
        "106": 6.852781295776367,
        "107": 16.105445861816406,
        "108": -2.2298104763031006,
        "109": 14.849919319152832,
        "110": 12.081732749938965,
        "111": 5.577458381652832,
        "112": -7.903711318969727,
        "113": 2.1638245582580566,
        "114": 12.51416015625,
        "115": -6.057870864868164,
        "116": -3.1622769832611084,
        "117": 15.445413589477539,
        "118": -11.344062805175781,
        "119": 19.498449325561523,
        "120": 1.2452481985092163,
        "121": 15.877655982971191,
        "122": -0.6422742605209351,
        "123": 17.894371032714844,
        "124": 18.51606559753418,
        "125": 3.8730247020721436,
        "126": -15.689104080200195,
        "127": 5.272400379180908,
        "128": -10.116860389709473,
        "129": 15.908406257629395,
        "130": 15.906386375427246,
        "131": 17.664257049560547,
        "132": -11.691399574279785,
        "133": 18.16555404663086,
        "134": -3.484729766845703,
        "135": -0.28802651166915894,
        "136": 9.640143394470215,
        "137": -10.761134147644043,
        "138": -12.508712768554688,
        "139": 1.0845882892608643,
        "140": -4.113940238952637,
        "141": -3.311356782913208,
        "142": -6.8592963218688965,
        "143": -3.737391471862793,
        "144": 1.5095330476760864,
        "145": 6.855871677398682,
        "146": -0.005576949566602707,
        "147": -13.6783447265625,
        "148": -0.20885930955410004,
        "149": -0.3413524031639099,
        "150": -7.44395112991333,
        "151": 2.547792434692383,
        "152": -4.23331880569458,
        "153": -2.8638343811035156,
        "154": -9.176346778869629,
        "155": -9.91745376586914,
        "156": 19.491615295410156,
        "157": -1.1178452968597412,
        "158": -6.559659481048584,
        "159": -5.509847640991211,
        "160": -9.23338794708252,
        "161": -4.113839149475098,
        "162": -2.9389119148254395,
        "163": -6.750368118286133,
        "164": -15.22708797454834,
        "165": -11.957763671875,
        "166": -6.906585216522217,
        "167": -2.885266065597534,
        "168": -7.909487247467041,
        "169": -11.80526065826416,
        "170": -5.649503707885742,
        "171": -9.357735633850098,
        "172": -10.557400703430176,
        "173": 19.338542938232422,
        "174": 2.5101492404937744,
        "175": -0.8403536677360535,
        "176": -12.472395896911621,
        "177": -3.3533272743225098,
        "178": 2.3005623817443848,
        "179": 10.348834037780762,
        "180": -9.55434799194336,
        "181": -14.888672828674316,
        "182": -10.30130672454834,
        "183": 10.331987380981445,
        "184": -0.5717965364456177,
        "185": -8.64627456665039,
        "186": -0.32809391617774963,
        "187": -15.18575382232666,
        "188": -8.255075454711914,
        "189": -3.862386465072632,
        "190": -9.168136596679688,
        "191": -2.965688705444336,
        "192": -8.563236236572266,
        "193": -8.755470275878906,
        "194": -4.1727423667907715,
        "195": -9.0485200881958,
        "196": -6.6288957595825195,
        "197": -2.597414493560791,
        "198": -14.643625259399414,
        "199": -1.6802549362182617,
        "200": -5.944916248321533,
        "201": 9.949645042419434,
        "202": -9.82835865020752,
        "203": -4.730626106262207,
        "204": 1.5123276710510254,
        "205": 8.08752727508545,
        "206": -7.239599227905273,
        "207": -8.046741485595703,
        "208": -7.834892749786377,
        "209": 16.31792640686035,
        "210": -2.144123077392578,
        "211": -3.397310256958008,
        "212": -3.492358922958374,
        "213": -1.9089761972427368,
        "214": -8.501752853393555,
        "215": 2.433518886566162,
        "216": 2.0097455978393555,
        "217": 11.263683319091797,
        "218": 9.172547340393066,
        "219": -11.684858322143555,
        "220": -4.254373073577881,
        "221": -8.132200241088867,
        "222": -8.017436981201172,
        "223": -4.518898010253906,
        "224": -7.769855499267578,
        "225": -2.9794774055480957,
        "226": -9.301362037658691,
        "227": 16.39617156982422,
        "228": 0.08578848093748093,
        "229": -11.525227546691895,
        "230": -4.852415084838867,
        "231": -7.29777717590332,
        "232": -3.750074625015259,
        "233": -9.286445617675781,
        "234": -4.501296520233154,
        "235": -7.908074855804443,
        "236": -1.3191031217575073,
        "237": -1.3143765926361084,
        "238": -9.574336051940918,
        "239": -6.332449436187744,
        "240": -1.9628465175628662,
        "241": -10.14108657836914,
        "242": -10.04944896697998,
        "243": -8.421611785888672,
        "244": -11.20667839050293,
        "245": -6.79122257232666,
        "246": -10.169275283813477,
        "247": 13.934502601623535,
        "248": -7.577616214752197,
        "249": -7.750513553619385,
        "250": -7.618638038635254,
        "251": -8.013586044311523,
        "252": -7.963730335235596,
        "253": -6.0882086753845215,
        "254": -5.4353413581848145,
        "255": -3.760608196258545,
        "256": -1.1739916801452637,
        "257": 0.5588189363479614,
        "258": -3.0574612617492676,
        "259": -0.6962624788284302,
        "260": -3.915628433227539,
        "261": -0.7879827618598938,
        "262": -0.2777913510799408,
        "263": -5.334052562713623
    },
    "y": {
        "0": -4.306889057159424,
        "1": 2.4184179306030273,
        "2": -9.956250190734863,
        "3": -5.740766525268555,
        "4": -4.790846824645996,
        "5": -8.15465259552002,
        "6": 6.381347179412842,
        "7": -6.650630474090576,
        "8": -2.3326659202575684,
        "9": -10.389246940612793,
        "10": -0.03403949365019798,
        "11": -0.8416275978088379,
        "12": -3.2353427410125732,
        "13": -0.32297149300575256,
        "14": 5.0091352462768555,
        "15": -0.8465414643287659,
        "16": 0.631625771522522,
        "17": 17.430986404418945,
        "18": 1.666228175163269,
        "19": 0.15593105554580688,
        "20": 6.461092472076416,
        "21": -10.789669036865234,
        "22": -8.882631301879883,
        "23": 1.6364023685455322,
        "24": 0.533145010471344,
        "25": 1.9875789880752563,
        "26": -1.3940260410308838,
        "27": 0.2342032492160797,
        "28": -7.61863374710083,
        "29": 1.4273362159729004,
        "30": -6.27547550201416,
        "31": -9.211804389953613,
        "32": -4.331857204437256,
        "33": 3.087472915649414,
        "34": 2.470369815826416,
        "35": -9.971585273742676,
        "36": -8.077604293823242,
        "37": 0.690123438835144,
        "38": -3.3563055992126465,
        "39": -4.2188897132873535,
        "40": -3.8476271629333496,
        "41": -7.533361434936523,
        "42": -4.1354169845581055,
        "43": 9.76529598236084,
        "44": -5.87007474899292,
        "45": -6.07473087310791,
        "46": 0.4593905806541443,
        "47": 11.673909187316895,
        "48": -6.449310779571533,
        "49": 11.060150146484375,
        "50": -8.295675277709961,
        "51": -7.572315216064453,
        "52": -3.314309597015381,
        "53": -0.7085980176925659,
        "54": -7.186641693115234,
        "55": -7.042220592498779,
        "56": -8.793241500854492,
        "57": 1.3464375734329224,
        "58": -1.6985949277877808,
        "59": -11.457075119018555,
        "60": -2.175461769104004,
        "61": 3.2734487056732178,
        "62": -2.969395875930786,
        "63": -0.3986892104148865,
        "64": 8.219697952270508,
        "65": -6.602794170379639,
        "66": 0.6533457636833191,
        "67": 10.071426391601562,
        "68": 9.82986068725586,
        "69": -11.342432022094727,
        "70": -6.000223159790039,
        "71": -2.3655202388763428,
        "72": 17.26102066040039,
        "73": -4.483091354370117,
        "74": -5.037186145782471,
        "75": 4.682310581207275,
        "76": -9.359999656677246,
        "77": 15.233586311340332,
        "78": 9.206169128417969,
        "79": -11.60775375366211,
        "80": 1.8608983755111694,
        "81": -4.8266425132751465,
        "82": -8.18925666809082,
        "83": -10.735220909118652,
        "84": 4.134460926055908,
        "85": 12.552899360656738,
        "86": -7.315394878387451,
        "87": -1.7523834705352783,
        "88": 8.695998191833496,
        "89": -6.5313310623168945,
        "90": -3.343258857727051,
        "91": -2.1186065673828125,
        "92": 4.079190731048584,
        "93": 6.308185577392578,
        "94": -5.456481456756592,
        "95": -6.224247455596924,
        "96": 14.4931640625,
        "97": -3.3080391883850098,
        "98": -0.7905216217041016,
        "99": 6.798697471618652,
        "100": -2.9307050704956055,
        "101": 8.879792213439941,
        "102": -1.7794501781463623,
        "103": 3.0358684062957764,
        "104": 12.279351234436035,
        "105": -1.6154212951660156,
        "106": 6.5420122146606445,
        "107": 3.060328483581543,
        "108": 17.514606475830078,
        "109": -5.005454063415527,
        "110": -6.023499011993408,
        "111": -2.2744457721710205,
        "112": -5.732197284698486,
        "113": -0.4448012709617615,
        "114": -5.212326526641846,
        "115": -5.092454433441162,
        "116": 4.813259124755859,
        "117": -3.079521417617798,
        "118": 12.257091522216797,
        "119": 6.436526298522949,
        "120": 11.036542892456055,
        "121": 1.9826158285140991,
        "122": -12.713244438171387,
        "123": -1.8996633291244507,
        "124": 0.8613170981407166,
        "125": 9.202741622924805,
        "126": 4.201564788818359,
        "127": -1.376875877380371,
        "128": 14.82073974609375,
        "129": 2.3176872730255127,
        "130": -5.629481315612793,
        "131": -4.392310619354248,
        "132": -6.195339202880859,
        "133": 0.23667317628860474,
        "134": -2.7703449726104736,
        "135": 0.8497104048728943,
        "136": -3.182893991470337,
        "137": 11.42435073852539,
        "138": 13.700004577636719,
        "139": 6.953529357910156,
        "140": 10.437752723693848,
        "141": 4.878171443939209,
        "142": 6.640218734741211,
        "143": -2.4818224906921387,
        "144": 6.903251647949219,
        "145": 6.623412132263184,
        "146": -11.21207046508789,
        "147": -7.357354640960693,
        "148": 6.662934303283691,
        "149": -1.269543170928955,
        "150": 1.1588785648345947,
        "151": 7.588347434997559,
        "152": 5.524259567260742,
        "153": 13.183513641357422,
        "154": 14.368337631225586,
        "155": -4.481541156768799,
        "156": -8.571040153503418,
        "157": 13.342568397521973,
        "158": 8.659006118774414,
        "159": 4.5687456130981445,
        "160": -3.457644462585449,
        "161": 13.7088623046875,
        "162": 13.079998970031738,
        "163": 3.0871798992156982,
        "164": -8.85769271850586,
        "165": 11.441939353942871,
        "166": 2.109684944152832,
        "167": 5.9926862716674805,
        "168": -4.574958324432373,
        "169": 12.202103614807129,
        "170": 3.9986965656280518,
        "171": -3.1254491806030273,
        "172": -0.6244853138923645,
        "173": -8.569098472595215,
        "174": 7.00757360458374,
        "175": 8.384675979614258,
        "176": -7.351805686950684,
        "177": -14.060930252075195,
        "178": -2.6887497901916504,
        "179": 10.988958358764648,
        "180": 2.934771776199341,
        "181": -2.4768967628479004,
        "182": 2.9864554405212402,
        "183": 10.875655174255371,
        "184": 3.244234085083008,
        "185": 4.022868633270264,
        "186": 3.9113452434539795,
        "187": -1.8643500804901123,
        "188": 2.8757057189941406,
        "189": 5.947203636169434,
        "190": 5.889116287231445,
        "191": -12.743487358093262,
        "192": 1.827643632888794,
        "193": 10.24898910522461,
        "194": -8.468609809875488,
        "195": 6.311095237731934,
        "196": -3.2080836296081543,
        "197": -10.239873886108398,
        "198": -2.0406668186187744,
        "199": -14.211382865905762,
        "200": 0.09707599878311157,
        "201": 7.71173620223999,
        "202": 1.762228012084961,
        "203": 0.04435718059539795,
        "204": 9.924609184265137,
        "205": -12.179279327392578,
        "206": 0.4743535816669464,
        "207": -0.1202654018998146,
        "208": 10.772720336914062,
        "209": 5.831446647644043,
        "210": -13.415987968444824,
        "211": -5.5344133377075195,
        "212": 16.806699752807617,
        "213": -14.453453063964844,
        "214": 5.820963382720947,
        "215": -16.4677677154541,
        "216": 10.011592864990234,
        "217": 9.402751922607422,
        "218": 8.279195785522461,
        "219": 13.029378890991211,
        "220": -9.201813697814941,
        "221": -2.930741786956787,
        "222": -15.76587963104248,
        "223": -11.172613143920898,
        "224": 10.161605834960938,
        "225": -10.82960033416748,
        "226": 12.615099906921387,
        "227": 5.547525405883789,
        "228": -3.8231756687164307,
        "229": 1.1572661399841309,
        "230": -11.080177307128906,
        "231": -2.0715243816375732,
        "232": -14.011239051818848,
        "233": 1.0742266178131104,
        "234": -3.5291147232055664,
        "235": 10.123689651489258,
        "236": -7.563082695007324,
        "237": -7.567505836486816,
        "238": 0.10833972692489624,
        "239": -1.7100982666015625,
        "240": -7.376716136932373,
        "241": -15.532746315002441,
        "242": -0.2542880177497864,
        "243": -0.20218683779239655,
        "244": -15.214681625366211,
        "245": -14.384077072143555,
        "246": -15.483842849731445,
        "247": 6.194426536560059,
        "248": -14.564838409423828,
        "249": -1.5109409093856812,
        "250": -13.834895133972168,
        "251": -14.416693687438965,
        "252": -15.269255638122559,
        "253": -16.949159622192383,
        "254": -17.305625915527344,
        "255": -18.383201599121094,
        "256": -20.15699005126953,
        "257": -19.72146224975586,
        "258": -16.770174026489258,
        "259": -19.461149215698242,
        "260": -18.307329177856445,
        "261": -19.24874496459961,
        "262": -18.860515594482422,
        "263": -17.66147804260254
    },
    "title": {
        "0": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "1": "ART: Automatic multi-step reasoning and tool-use for large language models",
        "2": "Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning",
        "3": "Stable and low-precision training for large-scale vision-language models",
        "4": "QLoRA: Efficient Finetuning of Quantized LLMs",
        "5": "GATED STATE SPACES",
        "6": "The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants",
        "7": "MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers",
        "8": "Scaling Expert Language Models with Unsupervised Domain Discovery",
        "9": "Scaling Laws for Generative Mixed-Modal Language Models",
        "10": "Toolformer: Language Models Can Teach Themselves to Use Tools",
        "11": "LIMA: Less Is More for Alignment",
        "12": "Dictionary-based Phrase-level Prompting of Large Language Models for Machine Translation",
        "13": "XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models",
        "14": "Mixture of Prompt Experts for Generalizable and Interpretable Question Answering",
        "15": "Representation Deficiency in Masked Language Modeling",
        "16": "Shepherd: A Critic for Language Model Generation",
        "17": "Translate to Disambiguate: Zero-shot Multilingual Word Sense Disambiguation with Pretrained Language Models",
        "18": "Generative Models as a Complex Systems Science: How can we make sense of large language model behavior?",
        "19": "Trusting Your Evidence: Hallucinate Less with Context-aware Decoding",
        "20": "The Gender-GAP Pipeline: A Gender-Aware Polyglot Pipeline for Gender Characterisation in 55 Languages",
        "21": "Retrieval-Augmented Multimodal Language Modeling",
        "22": "CiT: Curation in Training for Effective Vision-Language Data",
        "23": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
        "24": "SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore",
        "25": "Revisiting Machine Translation for Cross-lingual Classification",
        "26": "Self-Alignment with Instruction Backtranslation",
        "27": "Contrastive Decoding: Open-ended Text Generation as Optimization",
        "28": "Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations",
        "29": "Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection",
        "30": "Prompting Language Models for Linguistic Structure",
        "31": "Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models",
        "32": "OPT: Open Pre-trained Transformer Language Models",
        "33": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters",
        "34": "ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning",
        "35": "Improving Policy Learning via Language Dynamics Distillation",
        "36": "Natural Language to Code Translation with Execution",
        "37": "Language Contamination Helps Explains the Cross-lingual Capabilities of English Pretrained Models",
        "38": "CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation",
        "39": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale",
        "40": "In-context Examples Selection for Machine Translation",
        "41": "InCoder: A Generative Model for Code Infilling and Synthesis",
        "42": "The case for 4-bit precision: k-bit Inference Scaling Laws",
        "43": "UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models",
        "44": "Nearest Neighbor Zero-Shot Inference",
        "45": "LegoNN: Building Modular Encoder-Decoder Models",
        "46": "Training Trajectories of Language Models Across Scales",
        "47": "Improving Passage Retrieval with Zero-Shot Question Generation",
        "48": "Prompt-free and Efficient Few-shot Learning with Language Models",
        "49": "Questions Are All You Need to Train a Dense Passage Retriever",
        "50": "One Embedder, Any Task: Instruction-Finetuned Text Embeddings",
        "51": "Mega: Moving Average Equipped Gated Attention",
        "52": "Efficient Large Scale Language Modeling with Mixtures of Experts",
        "53": "M2D2: A Massively Multi-Domain Language Modeling Dataset",
        "54": "Selective Annotation Makes Language Models Better Few-Shot Learners",
        "55": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
        "56": "DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation",
        "57": "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models",
        "58": "Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models",
        "59": "AGRO: Adversarial Discovery of Error-prone groups for Robust Optimization",
        "60": "On the Role of Bidirectionality in Language Model Pre-Training",
        "61": "Logical Satisfiability of Counterfactuals for Faithful Explanations in NLI",
        "62": "Stop: A Dataset for Spoken Task Oriented Semantic Parsing",
        "63": "Nonparametric Masked Language Modeling",
        "64": "CREPE: Open-Domain Question Answering with False Presuppositions",
        "65": "Toward Human Readable Prompt Tuning: Kubrick's The Shining is a good movie, and a good prompt too?",
        "66": "Language Contamination Explains the Cross-lingual Capabilities of English Pretrained Models",
        "67": "RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering",
        "68": "Bayesian Deep Learning for Interactive Community Question Answering",
        "69": "CM3: A Causal Masked Multimodal Model of the Internet",
        "70": "Demystifying Prompts in Language Models via Perplexity Estimation",
        "71": "Few-shot Mining of Naturally Occurring Inputs and Outputs",
        "72": "FEWS: Large-Scale, Low-Shot Word Sense Disambiguation with the Dictionary",
        "73": "8-bit Optimizers via Block-wise Quantization",
        "74": "Few-shot Learning with Multilingual Language Models",
        "75": "FaVIQ: FAct Verification from Information-seeking Questions",
        "76": "VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding",
        "77": "DESCGEN: A Distantly Supervised Datasetfor Generating Entity Descriptions",
        "78": "Language Grounding with 3D Objects",
        "79": "Learning Better Structured Representations Using Low-rank Adaptive Label Smoothing",
        "80": "Quantifying Adaptability in Pre-trained Language Models with 500 Tasks",
        "81": "Few-shot Learning with Multilingual Generative Language Models",
        "82": "MetaICL: Learning to Learn In Context",
        "83": "SILG: The Multi-environment Symbolic Interactive Language Grounding Benchmark",
        "84": "Prompting Contrastive Explanations for Commonsense Reasoning Tasks",
        "85": "Question Answering Infused Pre-training of General-Purpose Contextualized Representations",
        "86": "Muppet: Massive Multi-task Representations with Pre-Finetuning",
        "87": "DEMix Layers: Disentangling Domains for Modular Language Modeling",
        "88": "Surface Form Competition: Why the Highest Probability Answer Isn\u2019t Always Right",
        "89": "Luna: Linear Unified Nested Attention",
        "90": "BASE Layers: Simplifying Training of Large, Sparse Models",
        "91": "BitextEdit: Automatic Bitext Editing for Improved Low-Resource Machine Translation",
        "92": "Bilingual Lexicon Induction via Unsupervised Bitext Construction and Word Alignment",
        "93": "Inducing Semantic Roles Without Syntax",
        "94": "HTLM: Hyper-Text Pre-Training and Prompting of Language Models",
        "95": "Noisy Channel Language Model Prompting for Few-Shot Text Classification",
        "96": "Multilingual Autoregressive Entity Linking",
        "97": "Efficient Large Scale Language Modeling with Mixtures of Experts",
        "98": "Detecting Hallucinated Content in Conditional Neural Sequence Generation",
        "99": "QANom: Question-Answer driven SRL for Nominalizations",
        "100": "Nearest Neighbor Machine Translation",
        "101": "AmbigQA: Answering Ambiguous Open-domain Questions",
        "102": "Simple and Effective Retrieve-Edit-Rerank Text Generation",
        "103": "Aligned Cross Entropy for Non-Autoregressive Machine Translation",
        "104": "Active Learning for Coreference Resolution using Discrete Annotation",
        "105": "Multilingual Denoising Pre-training for Neural Machine Translation",
        "106": "Learning to Model and Ignore Dataset Bias with Mixed Capacity Ensembles",
        "107": "Semi-Autoregressive Training Improves Mask-Predict Decoding",
        "108": "Moving Down the Long Tail of Word Sense Disambiguation with Gloss Informed Bi-encoders",
        "109": "DeLighT: Very Deep and Light-weight Transformer",
        "110": "Better Fine-Tuning by Reducing Representational Collapse",
        "111": "Pre-training via Paraphrasing",
        "112": "Grounded Adaptation for Zero-shot Executable Semantic Parsing",
        "113": "An Information Bottleneck Approach for Controlling Conciseness in Rationale Extraction",
        "114": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning",
        "115": "Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic Parsing",
        "116": "Controlled Crowdsourcing for High-Quality QA-SRL Annotation",
        "117": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "118": "BERT for Coreference Resolution: Baselines and Analysis",
        "119": "Evaluating Gender Bias in Machine Translation",
        "120": "Knowledge Guided Text Retrieval and Reading for Open Domain Question Answering",
        "121": "Constant-Time Machine Translation with Conditional Masked Language Models",
        "122": "Vision-and-Dialog Navigation",
        "123": "Better Character Language Modeling through Morphology",
        "124": "Emerging Cross-lingual Structure in Pretrained Language Models",
        "125": "A Discrete Hard EM Approach for Weakly Supervised Question Answering",
        "126": "Enforcing Encoder-Decoder Modularity in Sequence-to-Sequence Models",
        "127": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
        "128": "Zero-shot Entity Linking with Dense Entity Retrieval",
        "129": "Mask-Predict: Parallel Decoding of Conditional Masked Language Models",
        "130": "Transformers with convolutional context for ASR",
        "131": "Sparse Networks from Scratch: Faster Training without Losing Performance",
        "132": "Learning Programmatic Idioms for Scalable Semantic Parsing",
        "133": "Unsupervised Cross-lingual Representation Learning at Scale",
        "134": "Span-based Hierarchical Semantic Parsing for Task-Oriented Dialog",
        "135": "E3: Entailment-driven Extracting and Editing for Conversational Machine Reading",
        "136": "Generalization through Memorization: Nearest Neighbor Language Models",
        "137": "SpanBERT: Improving Pre-training by Representing and Predicting Spans",
        "138": "The Referential Reader: A Recurrent Entity Network for Anaphora Resolution",
        "139": "Compositional Questions Do Not Necessitate Multi-hop Reasoning",
        "140": "Cloze-driven Pretraining of Self-attention Networks",
        "141": "Crowdsourcing a High-Quality Gold Standard for QA-SRL",
        "142": "Iterative Search for Weakly Supervised Semantic Parsing",
        "143": "Improving Semantic Parsing for Task Oriented Dialog",
        "144": "Multi-hop Reading Comprehension through Question Decomposition and Rescoring",
        "145": "Don\u2019t Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases",
        "146": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks",
        "147": "JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation",
        "148": "SimpleQuestions Nearly Solved: A New Upperbound and Baseline Approach",
        "149": "Adversarial Example Generation with Syntactically Controlled Paraphrase Networks",
        "150": "Neural Semantic Parsing",
        "151": "QuAC: Question Answering in Context",
        "152": "Large-Scale QA-SRL Parsing",
        "153": "Deep Contextualized Word Representations",
        "154": "Ultra-Fine Entity Typing",
        "155": "NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System",
        "156": "Long Short-Term Memory as a Dynamically Computed Element-wise Weighted Sum",
        "157": "pair2vec: Compositional Word-Pair Embeddings for Cross-Sentence Inference",
        "158": "Supervised Open Information Extraction",
        "159": "Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling",
        "160": "AllenNLP: A Deep Semantic Natural Language Processing Platform",
        "161": "Neural Metaphor Detection in Context",
        "162": "Dissecting Contextual Word Embeddings: Architecture and Representation",
        "163": "Syntactic Scaffolds for Semantic Structures",
        "164": "Recognizing and Imitating Programmer Style: Adversaries in Program Authorship Attribution",
        "165": "Higher-Order Coreference Resolution with Coarse-to-Fine Inference",
        "166": "Deep RNNs Encode Soft Hierarchical Syntax",
        "167": "Crowdsourcing Question-Answer Meaning Representations",
        "168": "Learning a Neural Semantic Parser from User Feedback",
        "169": "End-to-end Neural Coreference Resolution",
        "170": "Deep Semantic Role Labeling: What Works and What\u2019s Next",
        "171": "AllenNLP: A Deep Semantic Natural Language Processing Platform",
        "172": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation",
        "173": "Recurrent Additive Networks",
        "174": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
        "175": "Zero-Shot Relation Extraction via Reading Comprehension",
        "176": "Summarizing Source Code using a Neural Attention Model",
        "177": "Integrated Intelligence: Robot Instruction via Interactive Grounded Learning",
        "178": "Globally Coherent Text Generation with Neural Checklist Models",
        "179": "Commonly Uncommon: Semantic Sparsity in Situation Recognition",
        "180": "LSTM CCG Parsing",
        "181": "A Theme-Rewriting Approach for Generating Algebra Word Problems",
        "182": "Global Neural CCG Parsing with Optimality Guarantees",
        "183": "Situation Recognition: Visual Semantic Role Labeling for Image Understanding",
        "184": "Document-level Sentiment Inference with Social, Faction, and Discourse Context",
        "185": "Human-in-the-Loop Parsing",
        "186": "Event Detection and Factuality Assessment with Non-Expert Supervision",
        "187": "Personalized Mathematical Word Problem Generation",
        "188": "Joint A* CCG Parsing and Semantic Role Labelling",
        "189": "Question-Answer Driven Semantic Role Labeling: Using Natural Language to Annotate Natural Language",
        "190": "Semantic Parsing for Information Extraction",
        "191": "Robot Programming by Demonstration with situated spatial language understanding",
        "192": "Broad-coverage CCG Semantic Parsing with AMR",
        "193": "Extreme Extraction: Only One Hour per Relation",
        "194": "Mise en Place: Unsupervised Interpretation of Instructional Recipes",
        "195": "Scalable Semantic Parsing with Partial Ontologies",
        "196": "Introduction to the special issue on learning semantics",
        "197": "Grounding Antonym Adjective Pairs through Interaction",
        "198": "Learning to Automatically Solve Algebra Word Problems",
        "199": "Learning from Unscripted Deictic Gesture and Language for Human-Robot Interactions",
        "200": "Context-dependent Semantic Parsing for Time Expressions",
        "201": "See No Evil, Say No Evil: Description Generation from Densely Labeled Images",
        "202": "Morpho-syntactic Lexical Generalization for CCG Semantic Parsing",
        "203": "D\u00e9duction et application d'une grammaire non contextuelle \u00e0 sujet cibl\u00e9",
        "204": "Open question answering over curated and extracted knowledge bases",
        "205": "Multi-Label Learning with Posterior Regularization",
        "206": "UW SPF: The University of Washington Semantic Parsing Framework",
        "207": "Semantic Parsing with Combinatory Categorial Grammars",
        "208": "Modeling Missing Data in Distant Supervision for Information Extraction",
        "209": "\u201cCan you give me another word for hyperbaric?\u201d: Improving speech translation using targeted clarification questions",
        "210": "Combining world and interaction models for human-robot collaborations",
        "211": "Lightly Supervised Learning of Procedural Dialog Systems",
        "212": "Automatic Idiom Identification in Wiktionary",
        "213": "Toward Unconstrained Gesture and Language Interfaces",
        "214": "Scaling Semantic Parsers with On-the-Fly Ontology Matching",
        "215": "3D Wikipedia",
        "216": "Paraphrase-Driven Learning for Open Question Answering",
        "217": "Learning to Relate Literal and Sentimental Descriptions of Visual Properties",
        "218": "Learning Distributions over Logical Forms for Referring Expression Generation",
        "219": "Joint Coreference Resolution and Named-Entity Linking with Multi-Pass Sieves",
        "220": "Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions",
        "221": "CSE 517 : Natural Language Processing New Quals Course !",
        "222": "Learning STRIPS Operators from Noisy and Incomplete Observations",
        "223": "Learning to Parse Natural Language Commands to a Robot Control System",
        "224": "Interactive Learning of Relation Extractors with Weak Supervision",
        "225": "A Joint Model of Language and Perception for Grounded Attribute Learning",
        "226": "Discriminative Learning for Joint Template Filling",
        "227": "Using syntactic and confusion network structure for out-of-vocabulary word detection",
        "228": "RevMiner: an extractive interface for navigating reviews on a smartphone",
        "229": "A Probabilistic Model of Syntactic and Semantic Acquisition from Child-Directed Utterances and their Meanings",
        "230": "Learning to Parse Natural Language to a Robot Execution System",
        "231": "Learning to map sentences to logical form",
        "232": "Interactive Learning and its Role in Pervasive Robotics",
        "233": "Lexical Generalization in CCG Grammar Induction for Semantic Parsing",
        "234": "Bootstrapping Semantic Parsers from Conversations",
        "235": "Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations",
        "236": "Reading between the Lines: Learning to Map High-Level Instructions to Commands",
        "237": "Reading Between the Lines : Learning to Map High-level Instructions to",
        "238": "Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification",
        "239": "Learning Context-Dependent Mappings from Sentences to Logical Form",
        "240": "Reinforcement Learning for Mapping Instructions to Actions",
        "241": "Multi-Agent Filtering with Infinitely Nested Beliefs",
        "242": "A Generative Model for Parsing Natural Language to Meaning Representations",
        "243": "Online Learning of Relaxed CCG Grammars for Parsing to Logical Form",
        "244": "Reasoning about Large Populations with Lifted Probabilistic Inference",
        "245": "Learning Probabilistic Relational Dynamics for Multiple Tasks",
        "246": "Logical Particle Filtering",
        "247": "Selective Phrase Pair Extraction for Improved Statistical Machine Translation",
        "248": "Learning Symbolic Models of Stochastic Domains",
        "249": "Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars",
        "250": "Learning Planning Rules in Noisy Stochastic Worlds",
        "251": "Learning and Planning with Probabilistic Relational Rules",
        "252": "Learning Probabilistic Relational Planning Rules",
        "253": "The user interface as an agent environment",
        "254": "User Interface Softbots",
        "255": "Programming by example: visual generalization in programming by example",
        "256": "Detecting and Correcting Misconceptions with Lifelike Avatars in 3 D Learning Environments",
        "257": "Towards Narrative-Centered Learning Environments",
        "258": "An Imprecise Mouse Gesture for the Fast Activation of Controls",
        "259": "Explanatory lifelike avatars: performing user-centered tasks in 3D learning environments",
        "260": "A visual medium for programmatic control of interactive applications",
        "261": "Habitable 3D Learning Environments for Situated Learning",
        "262": "Task-sensitive cinematography interfaces for interactive 3D learning environments",
        "263": "IBOTS: agent control through the user interface"
    },
    "year": {
        "0": 2023,
        "1": 2023,
        "2": 2023,
        "3": 2023,
        "4": 2023,
        "5": 2023,
        "6": 2023,
        "7": 2023,
        "8": 2023,
        "9": 2023,
        "10": 2023,
        "11": 2023,
        "12": 2023,
        "13": 2023,
        "14": 2023,
        "15": 2023,
        "16": 2023,
        "17": 2023,
        "18": 2023,
        "19": 2023,
        "20": 2023,
        "21": 2023,
        "22": 2023,
        "23": 2023,
        "24": 2023,
        "25": 2023,
        "26": 2023,
        "27": 2022,
        "28": 2022,
        "29": 2022,
        "30": 2022,
        "31": 2022,
        "32": 2022,
        "33": 2022,
        "34": 2022,
        "35": 2022,
        "36": 2022,
        "37": 2022,
        "38": 2022,
        "39": 2022,
        "40": 2022,
        "41": 2022,
        "42": 2022,
        "43": 2022,
        "44": 2022,
        "45": 2022,
        "46": 2022,
        "47": 2022,
        "48": 2022,
        "49": 2022,
        "50": 2022,
        "51": 2022,
        "52": 2022,
        "53": 2022,
        "54": 2022,
        "55": 2022,
        "56": 2022,
        "57": 2022,
        "58": 2022,
        "59": 2022,
        "60": 2022,
        "61": 2022,
        "62": 2022,
        "63": 2022,
        "64": 2022,
        "65": 2022,
        "66": 2022,
        "67": 2022,
        "68": 2022,
        "69": 2022,
        "70": 2022,
        "71": 2022,
        "72": 2021,
        "73": 2021,
        "74": 2021,
        "75": 2021,
        "76": 2021,
        "77": 2021,
        "78": 2021,
        "79": 2021,
        "80": 2021,
        "81": 2021,
        "82": 2021,
        "83": 2021,
        "84": 2021,
        "85": 2021,
        "86": 2021,
        "87": 2021,
        "88": 2021,
        "89": 2021,
        "90": 2021,
        "91": 2021,
        "92": 2021,
        "93": 2021,
        "94": 2021,
        "95": 2021,
        "96": 2021,
        "97": 2021,
        "98": 2020,
        "99": 2020,
        "100": 2020,
        "101": 2020,
        "102": 2020,
        "103": 2020,
        "104": 2020,
        "105": 2020,
        "106": 2020,
        "107": 2020,
        "108": 2020,
        "109": 2020,
        "110": 2020,
        "111": 2020,
        "112": 2020,
        "113": 2020,
        "114": 2020,
        "115": 2020,
        "116": 2019,
        "117": 2019,
        "118": 2019,
        "119": 2019,
        "120": 2019,
        "121": 2019,
        "122": 2019,
        "123": 2019,
        "124": 2019,
        "125": 2019,
        "126": 2019,
        "127": 2019,
        "128": 2019,
        "129": 2019,
        "130": 2019,
        "131": 2019,
        "132": 2019,
        "133": 2019,
        "134": 2019,
        "135": 2019,
        "136": 2019,
        "137": 2019,
        "138": 2019,
        "139": 2019,
        "140": 2019,
        "141": 2019,
        "142": 2019,
        "143": 2019,
        "144": 2019,
        "145": 2019,
        "146": 2019,
        "147": 2019,
        "148": 2018,
        "149": 2018,
        "150": 2018,
        "151": 2018,
        "152": 2018,
        "153": 2018,
        "154": 2018,
        "155": 2018,
        "156": 2018,
        "157": 2018,
        "158": 2018,
        "159": 2018,
        "160": 2018,
        "161": 2018,
        "162": 2018,
        "163": 2018,
        "164": 2018,
        "165": 2018,
        "166": 2018,
        "167": 2017,
        "168": 2017,
        "169": 2017,
        "170": 2017,
        "171": 2017,
        "172": 2017,
        "173": 2017,
        "174": 2017,
        "175": 2017,
        "176": 2016,
        "177": 2016,
        "178": 2016,
        "179": 2016,
        "180": 2016,
        "181": 2016,
        "182": 2016,
        "183": 2016,
        "184": 2016,
        "185": 2016,
        "186": 2015,
        "187": 2015,
        "188": 2015,
        "189": 2015,
        "190": 2015,
        "191": 2015,
        "192": 2015,
        "193": 2015,
        "194": 2015,
        "195": 2015,
        "196": 2014,
        "197": 2014,
        "198": 2014,
        "199": 2014,
        "200": 2014,
        "201": 2014,
        "202": 2014,
        "203": 2014,
        "204": 2014,
        "205": 2014,
        "206": 2013,
        "207": 2013,
        "208": 2013,
        "209": 2013,
        "210": 2013,
        "211": 2013,
        "212": 2013,
        "213": 2013,
        "214": 2013,
        "215": 2013,
        "216": 2013,
        "217": 2013,
        "218": 2013,
        "219": 2013,
        "220": 2013,
        "221": 2013,
        "222": 2012,
        "223": 2012,
        "224": 2012,
        "225": 2012,
        "226": 2012,
        "227": 2012,
        "228": 2012,
        "229": 2012,
        "230": 2012,
        "231": 2012,
        "232": 2012,
        "233": 2011,
        "234": 2011,
        "235": 2011,
        "236": 2010,
        "237": 2010,
        "238": 2010,
        "239": 2009,
        "240": 2009,
        "241": 2008,
        "242": 2008,
        "243": 2007,
        "244": 2007,
        "245": 2007,
        "246": 2007,
        "247": 2007,
        "248": 2007,
        "249": 2005,
        "250": 2005,
        "251": 2004,
        "252": 2004,
        "253": 2000,
        "254": 2000,
        "255": 2000,
        "256": 1999,
        "257": 1999,
        "258": 1999,
        "259": 1999,
        "260": 1999,
        "261": 1998,
        "262": 1998,
        "263": 1998
    },
    "cluster": {
        "0": 1,
        "1": 4,
        "2": 1,
        "3": 1,
        "4": 1,
        "5": 1,
        "6": 4,
        "7": 1,
        "8": 1,
        "9": 1,
        "10": 4,
        "11": 4,
        "12": 1,
        "13": 1,
        "14": 4,
        "15": 1,
        "16": 4,
        "17": 4,
        "18": 4,
        "19": 4,
        "20": 1,
        "21": 4,
        "22": 1,
        "23": 4,
        "24": 1,
        "25": 1,
        "26": 4,
        "27": 4,
        "28": 4,
        "29": 1,
        "30": 2,
        "31": 1,
        "32": 1,
        "33": 4,
        "34": 4,
        "35": 3,
        "36": 4,
        "37": 1,
        "38": 4,
        "39": 1,
        "40": 4,
        "41": 4,
        "42": 1,
        "43": 4,
        "44": 4,
        "45": 1,
        "46": 1,
        "47": 4,
        "48": 4,
        "49": 4,
        "50": 4,
        "51": 1,
        "52": 1,
        "53": 1,
        "54": 4,
        "55": 4,
        "56": 4,
        "57": 1,
        "58": 1,
        "59": 1,
        "60": 1,
        "61": 4,
        "62": 2,
        "63": 1,
        "64": 0,
        "65": 4,
        "66": 1,
        "67": 4,
        "68": 4,
        "69": 4,
        "70": 4,
        "71": 4,
        "72": 0,
        "73": 1,
        "74": 1,
        "75": 0,
        "76": 1,
        "77": 0,
        "78": 3,
        "79": 1,
        "80": 4,
        "81": 4,
        "82": 4,
        "83": 3,
        "84": 4,
        "85": 4,
        "86": 4,
        "87": 1,
        "88": 4,
        "89": 1,
        "90": 1,
        "91": 1,
        "92": 1,
        "93": 2,
        "94": 4,
        "95": 4,
        "96": 0,
        "97": 1,
        "98": 4,
        "99": 2,
        "100": 1,
        "101": 4,
        "102": 4,
        "103": 1,
        "104": 0,
        "105": 1,
        "106": 4,
        "107": 1,
        "108": 0,
        "109": 1,
        "110": 1,
        "111": 4,
        "112": 2,
        "113": 4,
        "114": 1,
        "115": 2,
        "116": 2,
        "117": 1,
        "118": 0,
        "119": 1,
        "120": 4,
        "121": 1,
        "122": 3,
        "123": 1,
        "124": 1,
        "125": 4,
        "126": 1,
        "127": 4,
        "128": 0,
        "129": 1,
        "130": 1,
        "131": 1,
        "132": 2,
        "133": 1,
        "134": 2,
        "135": 4,
        "136": 1,
        "137": 0,
        "138": 0,
        "139": 4,
        "140": 1,
        "141": 2,
        "142": 2,
        "143": 2,
        "144": 4,
        "145": 4,
        "146": 3,
        "147": 4,
        "148": 0,
        "149": 4,
        "150": 2,
        "151": 4,
        "152": 2,
        "153": 2,
        "154": 0,
        "155": 2,
        "156": 1,
        "157": 4,
        "158": 0,
        "159": 2,
        "160": 2,
        "161": 4,
        "162": 2,
        "163": 2,
        "164": 0,
        "165": 0,
        "166": 2,
        "167": 2,
        "168": 2,
        "169": 0,
        "170": 2,
        "171": 2,
        "172": 2,
        "173": 1,
        "174": 4,
        "175": 0,
        "176": 4,
        "177": 3,
        "178": 4,
        "179": 3,
        "180": 2,
        "181": 4,
        "182": 2,
        "183": 3,
        "184": 0,
        "185": 2,
        "186": 0,
        "187": 3,
        "188": 2,
        "189": 2,
        "190": 2,
        "191": 3,
        "192": 2,
        "193": 0,
        "194": 3,
        "195": 2,
        "196": 2,
        "197": 0,
        "198": 0,
        "199": 3,
        "200": 2,
        "201": 4,
        "202": 2,
        "203": 0,
        "204": 4,
        "205": 1,
        "206": 2,
        "207": 2,
        "208": 0,
        "209": 4,
        "210": 3,
        "211": 3,
        "212": 0,
        "213": 3,
        "214": 2,
        "215": 3,
        "216": 4,
        "217": 3,
        "218": 4,
        "219": 0,
        "220": 2,
        "221": 2,
        "222": 3,
        "223": 3,
        "224": 0,
        "225": 3,
        "226": 0,
        "227": 1,
        "228": 4,
        "229": 2,
        "230": 3,
        "231": 2,
        "232": 3,
        "233": 2,
        "234": 2,
        "235": 0,
        "236": 3,
        "237": 3,
        "238": 2,
        "239": 2,
        "240": 3,
        "241": 3,
        "242": 2,
        "243": 2,
        "244": 3,
        "245": 3,
        "246": 3,
        "247": 0,
        "248": 3,
        "249": 2,
        "250": 3,
        "251": 3,
        "252": 3,
        "253": 3,
        "254": 3,
        "255": 3,
        "256": 3,
        "257": 3,
        "258": 3,
        "259": 3,
        "260": 3,
        "261": 3,
        "262": 3,
        "263": 3
    },
    "authors": {
        "0": "Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, et al.",
        "1": "Bhargavi Paranjape, Scott M. Lundberg, Sameer Singh, Hanna Hajishirzi, Luke Zettlemoyer, et al.",
        "2": "L. Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, O. Yu. Golovneva, et al.",
        "3": "Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari S. Morcos, Ali Farhadi, et al.",
        "4": "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer",
        "5": "Luke Zettlemoyer",
        "6": "Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, et al.",
        "7": "L. Yu, Daniel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, et al.",
        "8": "Suchin Gururangan, Margaret Li, M. Lewis, Weijia Shi, Tim Althoff, et al.",
        "9": "Armen Aghajanyan, L. Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, et al.",
        "10": "Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, M. Lomeli, et al.",
        "11": "Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, et al.",
        "12": "Marjan Ghazvininejad, Hila Gonen, and Luke Zettlemoyer",
        "13": "Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, et al.",
        "14": "Chenglei Si, Weijia Shi, Chen Zhao, Luke Zettlemoyer, and Jordan L. Boyd-Graber",
        "15": "Yu Meng, Jitin Krishnan, Sinong Wang, Qifan Wang, Yuning Mao, et al.",
        "16": "Tianlu Wang, Ping Yu, Xiaoqing Tan, Sean O'Brien, Ramakanth Pasunuru, et al.",
        "17": "Haoqiang Kang, Terra Blevins, and Luke Zettlemoyer",
        "18": "Ari Holtzman, Peter West, and Luke Zettlemoyer",
        "19": "Weijia Shi, Xiaochuang Han, M. Lewis, Yulia Tsvetkov, Luke Zettlemoyer, et al.",
        "20": "Benjamin Muller, Belen Alastruey, Prangthip Hansanti, Elahe Kalbassi, C. Ropers, et al.",
        "21": "Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, J. Leskovec, et al.",
        "22": "Hu Xu, Saining Xie, Po-Yao (Bernie) Huang, Licheng Yu, Russ Howes, et al.",
        "23": "Sewon Min, Kalpesh Krishna, Xinxi Lyu, M. Lewis, Wen-tau Yih, et al.",
        "24": "Sewon Min, Suchin Gururangan, Eric Wallace, Hanna Hajishirzi, Noah A. Smith, et al.",
        "25": "Mikel Artetxe, Vedanuj Goswami, Shruti Bhosale, Angela Fan, and Luke Zettlemoyer",
        "26": "Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, et al.",
        "27": "Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, et al.",
        "28": "Xinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer, and Hannaneh Hajishirzi",
        "29": "Suchin Gururangan, Dallas Card, Sarah K. Drier, E. K. Gade, Leroy Z. Wang, et al.",
        "30": "Terra Blevins, Hila Gonen, and Luke Zettlemoyer",
        "31": "Terra Blevins, Hila Gonen, and Luke Zettlemoyer",
        "32": "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, et al.",
        "33": "Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, et al.",
        "34": "O. Yu. Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, et al.",
        "35": "Victor Zhong, Jesse Mu, Luke Zettlemoyer, Edward Grefenstette, and Tim Rocktaschel",
        "36": "Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I. Wang",
        "37": "Terra Blevins and Luke Zettlemoyer",
        "38": "Tanay Dixit, Bhargavi Paranjape, Hannaneh Hajishirzi, and Luke Zettlemoyer",
        "39": "Tim Dettmers, M. Lewis, Younes Belkada, and Luke Zettlemoyer",
        "40": "Sweta Agrawal, Chunting Zhou, M. Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad",
        "41": "Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida I. Wang, Eric Wallace, et al.",
        "42": "Tim Dettmers and Luke Zettlemoyer",
        "43": "Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, et al.",
        "44": "Weijia Shi, Julian Michael, Suchin Gururangan, and Luke Zettlemoyer",
        "45": "Siddharth Dalmia, Dmytro Okhonko, M. Lewis, Sergey Edunov, Shinji Watanabe, et al.",
        "46": "M. Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, et al.",
        "47": "Devendra Singh Sachan, M. Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, et al.",
        "48": "Rabeeh Karimi Mahabadi, Luke Zettlemoyer, J. Henderson, Marzieh Saeidi, Lambert Mathias, et al.",
        "49": "Devendra Singh Sachan, M. Lewis, Dani Yogatama, Luke Zettlemoyer, J. Pineau, et al.",
        "50": "Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, et al.",
        "51": "Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, et al.",
        "52": "Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, et al.",
        "53": "Machel Reid, Victor Zhong, Suchin Gururangan, and Luke Zettlemoyer",
        "54": "Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, et al.",
        "55": "Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, M. Lewis, et al.",
        "56": "Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, et al.",
        "57": "Kushal Tirumala, Aram H. Markosyan, Luke Zettlemoyer, and Armen Aghajanyan",
        "58": "Margaret Li, Suchin Gururangan, Tim Dettmers, M. Lewis, Tim Althoff, et al.",
        "59": "Bhargavi Paranjape, Pradeep Dasigi, Vivek Srikumar, Luke Zettlemoyer, and Hanna Hajishirzi",
        "60": "Mikel Artetxe, Jingfei Du, Naman Goyal, Luke Zettlemoyer, and Ves Stoyanov",
        "61": "Suzanna Sia, Anton Belyy, Amjad Almahairi, Madian Khabsa, Luke Zettlemoyer, et al.",
        "62": "Paden Tomasello, Po-chun Hsu, Akshat Shrivastava, Daniel Lazar, Duc Le, et al.",
        "63": "Sewon Min, Weijia Shi, M. Lewis, Xilun Chen, Wen-tau Yih, et al.",
        "64": "Xinyan Velocity Yu, Sewon Min, Luke Zettlemoyer, and Hannaneh Hajishirzi",
        "65": "Weijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman, Yulia Tsvetkov, et al.",
        "66": "Terra Blevins and Luke Zettlemoyer",
        "67": "Victor Zhong, Weijia Shi, Wen-tau Yih, and Luke Zettlemoyer",
        "68": "Luke Zettlemoyer and Ves Stoyanov",
        "69": "Armen Aghajanyan, Po-Yao (Bernie) Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, et al.",
        "70": "Hila Gonen, Srini Iyer, Terra Blevins, Noah A. Smith, and Luke Zettlemoyer",
        "71": "Mandar Joshi, Terra Blevins, M. Lewis, Daniel S. Weld, and Luke Zettlemoyer",
        "72": "Terra Blevins, Mandar Joshi, and Luke Zettlemoyer",
        "73": "Tim Dettmers, M. Lewis, Sam Shleifer, and Luke Zettlemoyer",
        "74": "Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, et al.",
        "75": "Jungsoo Park, Sewon Min, Jaewoo Kang, Luke Zettlemoyer, and Hannaneh Hajishirzi",
        "76": "Hu Xu, Gargi Ghosh, Po-Yao (Bernie) Huang, Prahal Arora, Masoumeh Aminzadeh, et al.",
        "77": "Weijia Shi, Mandar Joshi, and Luke Zettlemoyer",
        "78": "Jesse Thomason, Mohit Shridhar, Yonatan Bisk, Chris Paxton, and Luke Zettlemoyer",
        "79": "Asish Ghoshal, Xilun Chen, Sonal Gupta, Luke Zettlemoyer, and Yashar Mehdad",
        "80": "Belinda Z. Li, Jane A. Yu, Madian Khabsa, Luke Zettlemoyer, A. Halevy, et al.",
        "81": "Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, et al.",
        "82": "Sewon Min, M. Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi",
        "83": "Victor Zhong, Austin W. Hanjie, Sida Wang, Karthik Narasimhan, and Luke Zettlemoyer",
        "84": "Bhargavi Paranjape, Julian Michael, Marjan Ghazvininejad, Luke Zettlemoyer, and Hannaneh Hajishirzi",
        "85": "Robin Jia, M. Lewis, and Luke Zettlemoyer",
        "86": "Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, et al.",
        "87": "Suchin Gururangan, Michael Lewis, Ari Holtzman, Noah A. Smith, and Luke Zettlemoyer",
        "88": "Ari Holtzman, Peter West, Vered Schwartz, Yejin Choi, and Luke Zettlemoyer",
        "89": "Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, et al.",
        "90": "M. Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer",
        "91": "Eleftheria Briakou, Sida Wang, Luke Zettlemoyer, and Marjan Ghazvininejad",
        "92": "Freda Shi, Luke Zettlemoyer, and Sida I. Wang",
        "93": "Julian Michael and Luke Zettlemoyer",
        "94": "Armen Aghajanyan, Dmytro Okhonko, M. Lewis, Mandar Joshi, Hu Xu, et al.",
        "95": "Sewon Min, Michael Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer",
        "96": "Nicola De Cao, Ledell Yu Wu, Kashyap Popat, Mikel Artetxe, Naman Goyal, et al.",
        "97": "Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, et al.",
        "98": "Chunting Zhou, Jiatao Gu, Mona T. Diab, P. Guzm\u00e1n, Luke Zettlemoyer, et al.",
        "99": "Ayal Klein, Jonathan Mamou, Valentina Pyatkin, Daniela Stepanov, Hangfeng He, et al.",
        "100": "Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and M. Lewis",
        "101": "Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer",
        "102": "Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer",
        "103": "Marjan Ghazvininejad, Vladimir Karpukhin, Luke Zettlemoyer, and Omer Levy",
        "104": "Belinda Z. Li, Gabriel Stanovsky, and Luke Zettlemoyer",
        "105": "Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, et al.",
        "106": "Christopher Clark, Mark Yatskar, and Luke Zettlemoyer",
        "107": "Marjan Ghazvininejad, Omer Levy, and Luke Zettlemoyer",
        "108": "Terra Blevins and Luke Zettlemoyer",
        "109": "Sachin Mehta, Marjan Ghazvininejad, Srini Iyer, Luke Zettlemoyer, and Hannaneh Hajishirzi",
        "110": "Armen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Naman Goyal, Luke Zettlemoyer, et al.",
        "111": "M. Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida I. Wang, et al.",
        "112": "Victor Zhong, M. Lewis, Sida I. Wang, and Luke Zettlemoyer",
        "113": "Bhargavi Paranjape, Mandar Joshi, John Thickstun, Hannaneh Hajishirzi, and Luke Zettlemoyer",
        "114": "Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta",
        "115": "Xilun Chen, Asish Ghoshal, Yashar Mehdad, Luke Zettlemoyer, and S. Gupta",
        "116": "Paul Roit, Ayal Klein, Daniela Stepanov, Jonathan Mamou, Julian Michael, et al.",
        "117": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, et al.",
        "118": "Mandar Joshi, Omer Levy, Daniel S. Weld, and Luke Zettlemoyer",
        "119": "Gabriel Stanovsky, Noah A. Smith, and Luke Zettlemoyer",
        "120": "Sewon Min, Danqi Chen, Luke Zettlemoyer, and Hannaneh Hajishirzi",
        "121": "Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer",
        "122": "Jesse Thomason, Michael Murray, M. Cakmak, and Luke Zettlemoyer",
        "123": "Terra Blevins and Luke Zettlemoyer",
        "124": "Shijie Wu, Alexis Conneau, Haoran Li, Luke Zettlemoyer, and Veselin Stoyanov",
        "125": "Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer",
        "126": "Siddharth Dalmia, Abdel-rahman Mohamed, M. Lewis, Florian Metze, and Luke Zettlemoyer",
        "127": "M. Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdel-rahman Mohamed, et al.",
        "128": "Ledell Yu Wu, Fabio Petroni, Martin Josifoski, Sebastian Riedel, and Luke Zettlemoyer",
        "129": "Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer",
        "130": "Abdel-rahman Mohamed, Dmytro Okhonko, and Luke Zettlemoyer",
        "131": "Tim Dettmers and Luke Zettlemoyer",
        "132": "Srini Iyer, Alvin Cheung, and Luke Zettlemoyer",
        "133": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, et al.",
        "134": "Panupong Pasupat, S. Gupta, Karishma Mandyam, Rushin Shah, Michael Lewis, et al.",
        "135": "Victor Zhong and Luke Zettlemoyer",
        "136": "Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and M. Lewis",
        "137": "Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, et al.",
        "138": "Fei Liu, Luke Zettlemoyer, and Jacob Eisenstein",
        "139": "Sewon Min, Eric Wallace, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, et al.",
        "140": "Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, and Michael Auli",
        "141": "Paul Roit, Ayal Klein, Daniela Stepanov, Jonathan Mamou, Julian Michael, et al.",
        "142": "Pradeep Dasigi, Matt Gardner, Shikhar Murty, Luke Zettlemoyer, and E. Hovy",
        "143": "Arash Einolghozati, Panupong Pasupat, S. Gupta, Rushin Shah, Mrinal Mohit, et al.",
        "144": "Sewon Min, Victor Zhong, Luke Zettlemoyer, and Hannaneh Hajishirzi",
        "145": "Christopher Clark, Mark Yatskar, and Luke Zettlemoyer",
        "146": "Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, et al.",
        "147": "R. Agashe, Srini Iyer, and Luke Zettlemoyer",
        "148": "Michael Petrochuk and Luke Zettlemoyer",
        "149": "Mohit Iyyer, J. Wieting, Kevin Gimpel, and Luke Zettlemoyer",
        "150": "Matt Gardner, Pradeep Dasigi, Srini Iyer, Alane Suhr, and Luke Zettlemoyer",
        "151": "Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, et al.",
        "152": "Nicholas FitzGerald, Julian Michael, Luheng He, and Luke Zettlemoyer",
        "153": "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, et al.",
        "154": "Eunsol Choi, Omer Levy, Yejin Choi, and Luke Zettlemoyer",
        "155": "Xi Victoria Lin, Chenglong Wang, Luke Zettlemoyer, and Michael D. Ernst",
        "156": "Omer Levy, Kenton Lee, Nicholas FitzGerald, and Luke Zettlemoyer",
        "157": "Mandar Joshi, Eunsol Choi, Omer Levy, Daniel S. Weld, and Luke Zettlemoyer",
        "158": "Gabriel Stanovsky, Julian Michael, Luke Zettlemoyer, and Ido Dagan",
        "159": "Luheng He, Kenton Lee, Omer Levy, and Luke Zettlemoyer",
        "160": "Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, et al.",
        "161": "Ge Gao, Eunsol Choi, Yejin Choi, and Luke Zettlemoyer",
        "162": "Matthew E. Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih",
        "163": "Swabha Swayamdipta, Sam Thomson, Kenton Lee, Luke Zettlemoyer, Chris Dyer, et al.",
        "164": "Lucy Simko, Luke Zettlemoyer, and Tadayoshi Kohno",
        "165": "Kenton Lee, Luheng He, and Luke Zettlemoyer",
        "166": "Terra Blevins, Omer Levy, and Luke Zettlemoyer",
        "167": "Julian Michael, Gabriel Stanovsky, Luheng He, Ido Dagan, and Luke Zettlemoyer",
        "168": "Srini Iyer, Ioannis Konstas, Alvin Cheung, Jayant Krishnamurthy, and Luke Zettlemoyer",
        "169": "Kenton Lee, Luheng He, M. Lewis, and Luke Zettlemoyer",
        "170": "Luheng He, Kenton Lee, M. Lewis, and Luke Zettlemoyer",
        "171": "Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, et al.",
        "172": "Ioannis Konstas, Srini Iyer, Mark Yatskar, Yejin Choi, and Luke Zettlemoyer",
        "173": "Kenton Lee, Omer Levy, and Luke Zettlemoyer",
        "174": "Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer",
        "175": "Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer",
        "176": "Srini Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer",
        "177": "Luke Zettlemoyer and D. Fox",
        "178": "Chlo\u00e9 Kiddon, Luke Zettlemoyer, and Yejin Choi",
        "179": "Mark Yatskar, Vicente Ordonez, Luke Zettlemoyer, and Ali Farhadi",
        "180": "M. Lewis, Kenton Lee, and Luke Zettlemoyer",
        "181": "Rik Koncel-Kedziorski, Ioannis Konstas, Luke Zettlemoyer, and Hannaneh Hajishirzi",
        "182": "Kenton Lee, M. Lewis, and Luke Zettlemoyer",
        "183": "Mark Yatskar, Luke Zettlemoyer, and Ali Farhadi",
        "184": "Eunsol Choi, Hannah Rashkin, Luke Zettlemoyer, and Yejin Choi",
        "185": "Luheng He, Julian Michael, M. Lewis, and Luke Zettlemoyer",
        "186": "Kenton Lee, Yoav Artzi, Yejin Choi, and Luke Zettlemoyer",
        "187": "Oleksandr Polozov, Eleanor O'Rourke, Adam M. Smith, Luke Zettlemoyer, Sumit Gulwani, et al.",
        "188": "M. Lewis, Luheng He, and Luke Zettlemoyer",
        "189": "Luheng He, M. Lewis, and Luke Zettlemoyer",
        "190": "Eunsol Choi, T. Kwiatkowski, and Luke Zettlemoyer",
        "191": "Maxwell Forbes, Rajesh P. N. Rao, Luke Zettlemoyer, and M. Cakmak",
        "192": "Yoav Artzi, Kenton Lee, and Luke Zettlemoyer",
        "193": "Raphael Hoffmann, Luke Zettlemoyer, and Daniel S. Weld",
        "194": "Chlo\u00e9 Kiddon, Ganesa Thandavam Ponnuraj, Luke Zettlemoyer, and Yejin Choi",
        "195": "Eunsol Choi, T. Kwiatkowski, and Luke Zettlemoyer",
        "196": "Antoine Bordes, L. Bottou, Ronan Collobert, D. Roth, J. Weston, et al.",
        "197": "Maxwell Forbes, M. Chung, M. Cakmak, Luke Zettlemoyer, and Rajesh P. N. Rao",
        "198": "Nate Kushman, Luke Zettlemoyer, R. Barzilay, and Yoav Artzi",
        "199": "Cynthia Matuszek, Liefeng Bo, Luke Zettlemoyer, and D. Fox",
        "200": "Kenton Lee, Yoav Artzi, Jesse Dodge, and Luke Zettlemoyer",
        "201": "Mark Yatskar, Michel Galley, Lucy Vanderwende, and Luke Zettlemoyer",
        "202": "Adrienne Wang, T. Kwiatkowski, and Luke Zettlemoyer",
        "203": "Chris Quirk, P. Choudhury, Jurij Ganitkevic, and Luke Zettlemoyer",
        "204": "Anthony Fader, Luke Zettlemoyer, and Oren Etzioni",
        "205": "Xi Victoria Lin, Sameer Singh, Luheng He, B. Taskar, and Luke Zettlemoyer",
        "206": "Yoav Artzi and Luke Zettlemoyer",
        "207": "Yoav Artzi, Nicholas FitzGerald, and Luke Zettlemoyer",
        "208": "Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Etzioni",
        "209": "N. F. Ayan, Arindam Mandal, Michael W. Frandsen, Jing Zheng, Peter Blasco, et al.",
        "210": "Cynthia Matuszek, Andrzej Pronobis, Luke Zettlemoyer, and D. Fox",
        "211": "Svitlana Volkova, Pallavi Choudhury, Chris Quirk, W. Dolan, and Luke Zettlemoyer",
        "212": "Grace Muzny and Luke Zettlemoyer",
        "213": "Cynthia Matuszek, Liefeng Bo, Luke Zettlemoyer, and D. Fox",
        "214": "T. Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke Zettlemoyer",
        "215": "Bryan C. Russell, Ricardo Martin-Brualla, Daniel J. Butler, S. Seitz, and Luke Zettlemoyer",
        "216": "Anthony Fader, Luke Zettlemoyer, and Oren Etzioni",
        "217": "Mark Yatskar, Svitlana Volkova, Asli Celikyilmaz, W. Dolan, and Luke Zettlemoyer",
        "218": "Nicholas FitzGerald, Yoav Artzi, and Luke Zettlemoyer",
        "219": "Hannaneh Hajishirzi, Leila Zilles, Daniel S. Weld, and Luke Zettlemoyer",
        "220": "Yoav Artzi and Luke Zettlemoyer",
        "221": "Luke Zettlemoyer, D. Klein, and Russell T. Lewis",
        "222": "Kira Mour\u00e3o, Luke Zettlemoyer, Ronald P. A. Petrick, and Mark Steedman",
        "223": "Cynthia Matuszek, E. Herbst, Luke Zettlemoyer, and D. Fox",
        "224": "Daniel S. Weld, Luke Zettlemoyer, and Raphael Hoffmann",
        "225": "Cynthia Matuszek, Nicholas FitzGerald, Luke Zettlemoyer, Liefeng Bo, and D. Fox",
        "226": "Einat Minkov and Luke Zettlemoyer",
        "227": "Alex Marin, T. Kwiatkowski, Mari Ostendorf, and Luke Zettlemoyer",
        "228": "Jeff Huang, Oren Etzioni, Luke Zettlemoyer, Kevin Clark, and Christian Lee",
        "229": "T. Kwiatkowski, S. Goldwater, Luke Zettlemoyer, and Mark Steedman",
        "230": "Cynthia Matuszek, E. Herbst, Luke Zettlemoyer, and D. Fox",
        "231": "Luke Zettlemoyer",
        "232": "Cynthia Matuszek, Nicholas FitzGerald, E. Herbst, D. Fox, and Luke Zettlemoyer",
        "233": "T. Kwiatkowski, Luke Zettlemoyer, S. Goldwater, and Mark Steedman",
        "234": "Yoav Artzi and Luke Zettlemoyer",
        "235": "Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld",
        "236": "S. Branavan, Luke Zettlemoyer, and R. Barzilay",
        "237": "Luke Zettlemoyer and R. Barzilay",
        "238": "T. Kwiatkowski, Luke Zettlemoyer, S. Goldwater, and Mark Steedman",
        "239": "Luke Zettlemoyer and M. Collins",
        "240": "S. Branavan, Harr Chen, Luke Zettlemoyer, and R. Barzilay",
        "241": "Luke Zettlemoyer, Brian Milch, and L. Kaelbling",
        "242": "Wei Lu, H. Ng, Wee Sun Lee, and Luke Zettlemoyer",
        "243": "Luke Zettlemoyer and M. Collins",
        "244": "K. Kersting, Brian Milch, Luke Zettlemoyer, Michael Haimes, and L. Kaelbling",
        "245": "A. Deshpande, Brian Milch, Luke Zettlemoyer, and L. Kaelbling",
        "246": "Luke Zettlemoyer, H. Pasula, and L. Kaelbling",
        "247": "Luke Zettlemoyer and Robert C. Moore",
        "248": "L. Kaelbling, H. Pasula, and Luke Zettlemoyer",
        "249": "Luke Zettlemoyer and M. Collins",
        "250": "Luke Zettlemoyer, H. Pasula, and L. Kaelbling",
        "251": "H. Pasula, Luke Zettlemoyer, Leslie Pack, and Kaelbling",
        "252": "H. Pasula, Luke Zettlemoyer, and L. Kaelbling",
        "253": "R. Amant and Luke Zettlemoyer",
        "254": "R. Amant and Luke Zettlemoyer",
        "255": "R. Amant, H. Lieberman, R. Potter, and Luke Zettlemoyer",
        "256": "Jo\u00ebl P. Gr\u00e9goire, Luke Zettlemoyer, and James C. Lester",
        "257": "Bradford W. Mort, Charles B. Callaway, Luke Zettlemoyer, Seung Y. Lee, and James C. Lester",
        "258": "Martin S. Dulberg, R. Amant, and Luke Zettlemoyer",
        "259": "James C. Lester, Luke Zettlemoyer, Jo\u00ebl P. Gr\u00e9goire, and William H. Bares",
        "260": "Luke Zettlemoyer and R. Amant",
        "261": "William H. Bares, Luke Zettlemoyer, and James C. Lester",
        "262": "William H. Bares, Luke Zettlemoyer, Dennis W. Rodriguez, and James C. Lester",
        "263": "Luke Zettlemoyer, R. Amant, and Martin S. Dulberg"
    },
    "first_author": {
        "0": false,
        "1": false,
        "2": false,
        "3": false,
        "4": false,
        "5": true,
        "6": false,
        "7": false,
        "8": false,
        "9": false,
        "10": false,
        "11": false,
        "12": false,
        "13": false,
        "14": false,
        "15": false,
        "16": false,
        "17": false,
        "18": false,
        "19": false,
        "20": false,
        "21": false,
        "22": false,
        "23": false,
        "24": false,
        "25": false,
        "26": false,
        "27": false,
        "28": false,
        "29": false,
        "30": false,
        "31": false,
        "32": false,
        "33": false,
        "34": false,
        "35": false,
        "36": false,
        "37": false,
        "38": false,
        "39": false,
        "40": false,
        "41": false,
        "42": false,
        "43": false,
        "44": false,
        "45": false,
        "46": false,
        "47": false,
        "48": false,
        "49": false,
        "50": false,
        "51": false,
        "52": false,
        "53": false,
        "54": false,
        "55": false,
        "56": false,
        "57": false,
        "58": false,
        "59": false,
        "60": false,
        "61": false,
        "62": false,
        "63": false,
        "64": false,
        "65": false,
        "66": false,
        "67": false,
        "68": true,
        "69": false,
        "70": false,
        "71": false,
        "72": false,
        "73": false,
        "74": false,
        "75": false,
        "76": false,
        "77": false,
        "78": false,
        "79": false,
        "80": false,
        "81": false,
        "82": false,
        "83": false,
        "84": false,
        "85": false,
        "86": false,
        "87": false,
        "88": false,
        "89": false,
        "90": false,
        "91": false,
        "92": false,
        "93": false,
        "94": false,
        "95": false,
        "96": false,
        "97": false,
        "98": false,
        "99": false,
        "100": false,
        "101": false,
        "102": false,
        "103": false,
        "104": false,
        "105": false,
        "106": false,
        "107": false,
        "108": false,
        "109": false,
        "110": false,
        "111": false,
        "112": false,
        "113": false,
        "114": false,
        "115": false,
        "116": false,
        "117": false,
        "118": false,
        "119": false,
        "120": false,
        "121": false,
        "122": false,
        "123": false,
        "124": false,
        "125": false,
        "126": false,
        "127": false,
        "128": false,
        "129": false,
        "130": false,
        "131": false,
        "132": false,
        "133": false,
        "134": false,
        "135": false,
        "136": false,
        "137": false,
        "138": false,
        "139": false,
        "140": false,
        "141": false,
        "142": false,
        "143": false,
        "144": false,
        "145": false,
        "146": false,
        "147": false,
        "148": false,
        "149": false,
        "150": false,
        "151": false,
        "152": false,
        "153": false,
        "154": false,
        "155": false,
        "156": false,
        "157": false,
        "158": false,
        "159": false,
        "160": false,
        "161": false,
        "162": false,
        "163": false,
        "164": false,
        "165": false,
        "166": false,
        "167": false,
        "168": false,
        "169": false,
        "170": false,
        "171": false,
        "172": false,
        "173": false,
        "174": false,
        "175": false,
        "176": false,
        "177": true,
        "178": false,
        "179": false,
        "180": false,
        "181": false,
        "182": false,
        "183": false,
        "184": false,
        "185": false,
        "186": false,
        "187": false,
        "188": false,
        "189": false,
        "190": false,
        "191": false,
        "192": false,
        "193": false,
        "194": false,
        "195": false,
        "196": false,
        "197": false,
        "198": false,
        "199": false,
        "200": false,
        "201": false,
        "202": false,
        "203": false,
        "204": false,
        "205": false,
        "206": false,
        "207": false,
        "208": false,
        "209": false,
        "210": false,
        "211": false,
        "212": false,
        "213": false,
        "214": false,
        "215": false,
        "216": false,
        "217": false,
        "218": false,
        "219": false,
        "220": false,
        "221": true,
        "222": false,
        "223": false,
        "224": false,
        "225": false,
        "226": false,
        "227": false,
        "228": false,
        "229": false,
        "230": false,
        "231": true,
        "232": false,
        "233": false,
        "234": false,
        "235": false,
        "236": false,
        "237": true,
        "238": false,
        "239": true,
        "240": false,
        "241": true,
        "242": false,
        "243": true,
        "244": false,
        "245": false,
        "246": true,
        "247": true,
        "248": false,
        "249": true,
        "250": true,
        "251": false,
        "252": false,
        "253": false,
        "254": false,
        "255": false,
        "256": false,
        "257": false,
        "258": false,
        "259": false,
        "260": true,
        "261": false,
        "262": false,
        "263": true
    },
    "last_author": {
        "0": false,
        "1": false,
        "2": false,
        "3": false,
        "4": true,
        "5": false,
        "6": false,
        "7": false,
        "8": true,
        "9": true,
        "10": false,
        "11": false,
        "12": true,
        "13": false,
        "14": false,
        "15": true,
        "16": false,
        "17": true,
        "18": true,
        "19": false,
        "20": false,
        "21": false,
        "22": false,
        "23": false,
        "24": true,
        "25": true,
        "26": false,
        "27": false,
        "28": false,
        "29": false,
        "30": true,
        "31": true,
        "32": true,
        "33": false,
        "34": false,
        "35": false,
        "36": false,
        "37": true,
        "38": true,
        "39": true,
        "40": false,
        "41": false,
        "42": true,
        "43": false,
        "44": true,
        "45": false,
        "46": false,
        "47": true,
        "48": false,
        "49": false,
        "50": false,
        "51": true,
        "52": false,
        "53": true,
        "54": false,
        "55": true,
        "56": false,
        "57": false,
        "58": true,
        "59": false,
        "60": false,
        "61": false,
        "62": false,
        "63": true,
        "64": false,
        "65": true,
        "66": true,
        "67": true,
        "68": false,
        "69": true,
        "70": true,
        "71": true,
        "72": true,
        "73": true,
        "74": false,
        "75": false,
        "76": true,
        "77": true,
        "78": true,
        "79": false,
        "80": false,
        "81": false,
        "82": false,
        "83": true,
        "84": false,
        "85": true,
        "86": false,
        "87": true,
        "88": true,
        "89": true,
        "90": true,
        "91": false,
        "92": false,
        "93": true,
        "94": true,
        "95": true,
        "96": false,
        "97": false,
        "98": false,
        "99": false,
        "100": false,
        "101": true,
        "102": true,
        "103": false,
        "104": true,
        "105": true,
        "106": true,
        "107": true,
        "108": true,
        "109": false,
        "110": false,
        "111": true,
        "112": true,
        "113": true,
        "114": false,
        "115": false,
        "116": false,
        "117": false,
        "118": true,
        "119": true,
        "120": false,
        "121": true,
        "122": true,
        "123": true,
        "124": false,
        "125": true,
        "126": true,
        "127": true,
        "128": true,
        "129": true,
        "130": true,
        "131": true,
        "132": true,
        "133": false,
        "134": true,
        "135": true,
        "136": false,
        "137": false,
        "138": false,
        "139": true,
        "140": false,
        "141": false,
        "142": false,
        "143": true,
        "144": false,
        "145": true,
        "146": false,
        "147": true,
        "148": true,
        "149": true,
        "150": true,
        "151": true,
        "152": true,
        "153": true,
        "154": true,
        "155": false,
        "156": true,
        "157": true,
        "158": false,
        "159": true,
        "160": true,
        "161": true,
        "162": false,
        "163": false,
        "164": false,
        "165": true,
        "166": true,
        "167": true,
        "168": true,
        "169": true,
        "170": true,
        "171": true,
        "172": true,
        "173": true,
        "174": true,
        "175": true,
        "176": true,
        "177": false,
        "178": false,
        "179": false,
        "180": true,
        "181": false,
        "182": true,
        "183": false,
        "184": false,
        "185": true,
        "186": true,
        "187": false,
        "188": true,
        "189": true,
        "190": true,
        "191": false,
        "192": true,
        "193": false,
        "194": false,
        "195": true,
        "196": true,
        "197": false,
        "198": false,
        "199": false,
        "200": true,
        "201": true,
        "202": true,
        "203": true,
        "204": false,
        "205": true,
        "206": true,
        "207": true,
        "208": false,
        "209": false,
        "210": false,
        "211": true,
        "212": true,
        "213": false,
        "214": true,
        "215": true,
        "216": false,
        "217": true,
        "218": true,
        "219": true,
        "220": true,
        "221": false,
        "222": false,
        "223": false,
        "224": false,
        "225": false,
        "226": true,
        "227": true,
        "228": false,
        "229": false,
        "230": false,
        "231": false,
        "232": true,
        "233": false,
        "234": true,
        "235": false,
        "236": false,
        "237": false,
        "238": false,
        "239": false,
        "240": false,
        "241": false,
        "242": true,
        "243": false,
        "244": false,
        "245": false,
        "246": false,
        "247": false,
        "248": true,
        "249": false,
        "250": false,
        "251": false,
        "252": false,
        "253": true,
        "254": true,
        "255": true,
        "256": false,
        "257": false,
        "258": true,
        "259": false,
        "260": false,
        "261": false,
        "262": false,
        "263": false
    },
    "middle_author": {
        "0": true,
        "1": true,
        "2": true,
        "3": true,
        "4": false,
        "5": false,
        "6": true,
        "7": true,
        "8": false,
        "9": false,
        "10": true,
        "11": true,
        "12": false,
        "13": true,
        "14": true,
        "15": false,
        "16": true,
        "17": false,
        "18": false,
        "19": true,
        "20": true,
        "21": true,
        "22": true,
        "23": true,
        "24": false,
        "25": false,
        "26": true,
        "27": true,
        "28": true,
        "29": true,
        "30": false,
        "31": false,
        "32": false,
        "33": true,
        "34": true,
        "35": true,
        "36": true,
        "37": false,
        "38": false,
        "39": false,
        "40": true,
        "41": true,
        "42": false,
        "43": true,
        "44": false,
        "45": true,
        "46": true,
        "47": false,
        "48": true,
        "49": true,
        "50": true,
        "51": false,
        "52": true,
        "53": false,
        "54": true,
        "55": false,
        "56": true,
        "57": true,
        "58": false,
        "59": true,
        "60": true,
        "61": true,
        "62": true,
        "63": false,
        "64": true,
        "65": false,
        "66": false,
        "67": false,
        "68": false,
        "69": false,
        "70": false,
        "71": false,
        "72": false,
        "73": false,
        "74": true,
        "75": true,
        "76": false,
        "77": false,
        "78": false,
        "79": true,
        "80": true,
        "81": true,
        "82": true,
        "83": false,
        "84": true,
        "85": false,
        "86": true,
        "87": false,
        "88": false,
        "89": false,
        "90": false,
        "91": true,
        "92": true,
        "93": false,
        "94": false,
        "95": false,
        "96": true,
        "97": true,
        "98": true,
        "99": true,
        "100": true,
        "101": false,
        "102": false,
        "103": true,
        "104": false,
        "105": false,
        "106": false,
        "107": false,
        "108": false,
        "109": true,
        "110": true,
        "111": false,
        "112": false,
        "113": false,
        "114": true,
        "115": true,
        "116": true,
        "117": true,
        "118": false,
        "119": false,
        "120": true,
        "121": false,
        "122": false,
        "123": false,
        "124": true,
        "125": false,
        "126": false,
        "127": false,
        "128": false,
        "129": false,
        "130": false,
        "131": false,
        "132": false,
        "133": true,
        "134": false,
        "135": false,
        "136": true,
        "137": true,
        "138": true,
        "139": false,
        "140": true,
        "141": true,
        "142": true,
        "143": false,
        "144": true,
        "145": false,
        "146": true,
        "147": false,
        "148": false,
        "149": false,
        "150": false,
        "151": false,
        "152": false,
        "153": false,
        "154": false,
        "155": true,
        "156": false,
        "157": false,
        "158": true,
        "159": false,
        "160": false,
        "161": false,
        "162": true,
        "163": true,
        "164": true,
        "165": false,
        "166": false,
        "167": false,
        "168": false,
        "169": false,
        "170": false,
        "171": false,
        "172": false,
        "173": false,
        "174": false,
        "175": false,
        "176": false,
        "177": false,
        "178": true,
        "179": true,
        "180": false,
        "181": true,
        "182": false,
        "183": true,
        "184": true,
        "185": false,
        "186": false,
        "187": true,
        "188": false,
        "189": false,
        "190": false,
        "191": true,
        "192": false,
        "193": true,
        "194": true,
        "195": false,
        "196": false,
        "197": true,
        "198": true,
        "199": true,
        "200": false,
        "201": false,
        "202": false,
        "203": false,
        "204": true,
        "205": false,
        "206": false,
        "207": false,
        "208": true,
        "209": true,
        "210": true,
        "211": false,
        "212": false,
        "213": true,
        "214": false,
        "215": false,
        "216": true,
        "217": false,
        "218": false,
        "219": false,
        "220": false,
        "221": false,
        "222": true,
        "223": true,
        "224": true,
        "225": true,
        "226": false,
        "227": false,
        "228": true,
        "229": true,
        "230": true,
        "231": false,
        "232": false,
        "233": true,
        "234": false,
        "235": true,
        "236": true,
        "237": false,
        "238": true,
        "239": false,
        "240": true,
        "241": false,
        "242": false,
        "243": false,
        "244": true,
        "245": true,
        "246": false,
        "247": false,
        "248": false,
        "249": false,
        "250": false,
        "251": true,
        "252": true,
        "253": false,
        "254": false,
        "255": false,
        "256": true,
        "257": true,
        "258": false,
        "259": true,
        "260": false,
        "261": true,
        "262": true,
        "263": false
    },
    "author_of_interest": {
        "0": "Luke Zettlemoyer",
        "1": "Luke Zettlemoyer",
        "2": "Luke Zettlemoyer",
        "3": "Luke Zettlemoyer",
        "4": "Luke Zettlemoyer",
        "5": "Luke Zettlemoyer",
        "6": "Luke Zettlemoyer",
        "7": "Luke Zettlemoyer",
        "8": "Luke Zettlemoyer",
        "9": "Luke Zettlemoyer",
        "10": "Luke Zettlemoyer",
        "11": "Luke Zettlemoyer",
        "12": "Luke Zettlemoyer",
        "13": "Luke Zettlemoyer",
        "14": "Luke Zettlemoyer",
        "15": "Luke Zettlemoyer",
        "16": "Luke Zettlemoyer",
        "17": "Luke Zettlemoyer",
        "18": "Luke Zettlemoyer",
        "19": "Luke Zettlemoyer",
        "20": "Luke Zettlemoyer",
        "21": "Luke Zettlemoyer",
        "22": "Luke Zettlemoyer",
        "23": "Luke Zettlemoyer",
        "24": "Luke Zettlemoyer",
        "25": "Luke Zettlemoyer",
        "26": "Luke Zettlemoyer",
        "27": "Luke Zettlemoyer",
        "28": "Luke Zettlemoyer",
        "29": "Luke Zettlemoyer",
        "30": "Luke Zettlemoyer",
        "31": "Luke Zettlemoyer",
        "32": "Luke Zettlemoyer",
        "33": "Luke Zettlemoyer",
        "34": "Luke Zettlemoyer",
        "35": "Luke Zettlemoyer",
        "36": "Luke Zettlemoyer",
        "37": "Luke Zettlemoyer",
        "38": "Luke Zettlemoyer",
        "39": "Luke Zettlemoyer",
        "40": "Luke Zettlemoyer",
        "41": "Luke Zettlemoyer",
        "42": "Luke Zettlemoyer",
        "43": "Luke Zettlemoyer",
        "44": "Luke Zettlemoyer",
        "45": "Luke Zettlemoyer",
        "46": "Luke Zettlemoyer",
        "47": "Luke Zettlemoyer",
        "48": "Luke Zettlemoyer",
        "49": "Luke Zettlemoyer",
        "50": "Luke Zettlemoyer",
        "51": "Luke Zettlemoyer",
        "52": "Luke Zettlemoyer",
        "53": "Luke Zettlemoyer",
        "54": "Luke Zettlemoyer",
        "55": "Luke Zettlemoyer",
        "56": "Luke Zettlemoyer",
        "57": "Luke Zettlemoyer",
        "58": "Luke Zettlemoyer",
        "59": "Luke Zettlemoyer",
        "60": "Luke Zettlemoyer",
        "61": "Luke Zettlemoyer",
        "62": "Luke Zettlemoyer",
        "63": "Luke Zettlemoyer",
        "64": "Luke Zettlemoyer",
        "65": "Luke Zettlemoyer",
        "66": "Luke Zettlemoyer",
        "67": "Luke Zettlemoyer",
        "68": "Luke Zettlemoyer",
        "69": "Luke Zettlemoyer",
        "70": "Luke Zettlemoyer",
        "71": "Luke Zettlemoyer",
        "72": "Luke Zettlemoyer",
        "73": "Luke Zettlemoyer",
        "74": "Luke Zettlemoyer",
        "75": "Luke Zettlemoyer",
        "76": "Luke Zettlemoyer",
        "77": "Luke Zettlemoyer",
        "78": "Luke Zettlemoyer",
        "79": "Luke Zettlemoyer",
        "80": "Luke Zettlemoyer",
        "81": "Luke Zettlemoyer",
        "82": "Luke Zettlemoyer",
        "83": "Luke Zettlemoyer",
        "84": "Luke Zettlemoyer",
        "85": "Luke Zettlemoyer",
        "86": "Luke Zettlemoyer",
        "87": "Luke Zettlemoyer",
        "88": "Luke Zettlemoyer",
        "89": "Luke Zettlemoyer",
        "90": "Luke Zettlemoyer",
        "91": "Luke Zettlemoyer",
        "92": "Luke Zettlemoyer",
        "93": "Luke Zettlemoyer",
        "94": "Luke Zettlemoyer",
        "95": "Luke Zettlemoyer",
        "96": "Luke Zettlemoyer",
        "97": "Luke Zettlemoyer",
        "98": "Luke Zettlemoyer",
        "99": "Luke Zettlemoyer",
        "100": "Luke Zettlemoyer",
        "101": "Luke Zettlemoyer",
        "102": "Luke Zettlemoyer",
        "103": "Luke Zettlemoyer",
        "104": "Luke Zettlemoyer",
        "105": "Luke Zettlemoyer",
        "106": "Luke Zettlemoyer",
        "107": "Luke Zettlemoyer",
        "108": "Luke Zettlemoyer",
        "109": "Luke Zettlemoyer",
        "110": "Luke Zettlemoyer",
        "111": "Luke Zettlemoyer",
        "112": "Luke Zettlemoyer",
        "113": "Luke Zettlemoyer",
        "114": "Luke Zettlemoyer",
        "115": "Luke Zettlemoyer",
        "116": "Luke Zettlemoyer",
        "117": "Luke Zettlemoyer",
        "118": "Luke Zettlemoyer",
        "119": "Luke Zettlemoyer",
        "120": "Luke Zettlemoyer",
        "121": "Luke Zettlemoyer",
        "122": "Luke Zettlemoyer",
        "123": "Luke Zettlemoyer",
        "124": "Luke Zettlemoyer",
        "125": "Luke Zettlemoyer",
        "126": "Luke Zettlemoyer",
        "127": "Luke Zettlemoyer",
        "128": "Luke Zettlemoyer",
        "129": "Luke Zettlemoyer",
        "130": "Luke Zettlemoyer",
        "131": "Luke Zettlemoyer",
        "132": "Luke Zettlemoyer",
        "133": "Luke Zettlemoyer",
        "134": "Luke Zettlemoyer",
        "135": "Luke Zettlemoyer",
        "136": "Luke Zettlemoyer",
        "137": "Luke Zettlemoyer",
        "138": "Luke Zettlemoyer",
        "139": "Luke Zettlemoyer",
        "140": "Luke Zettlemoyer",
        "141": "Luke Zettlemoyer",
        "142": "Luke Zettlemoyer",
        "143": "Luke Zettlemoyer",
        "144": "Luke Zettlemoyer",
        "145": "Luke Zettlemoyer",
        "146": "Luke Zettlemoyer",
        "147": "Luke Zettlemoyer",
        "148": "Luke Zettlemoyer",
        "149": "Luke Zettlemoyer",
        "150": "Luke Zettlemoyer",
        "151": "Luke Zettlemoyer",
        "152": "Luke Zettlemoyer",
        "153": "Luke Zettlemoyer",
        "154": "Luke Zettlemoyer",
        "155": "Luke Zettlemoyer",
        "156": "Luke Zettlemoyer",
        "157": "Luke Zettlemoyer",
        "158": "Luke Zettlemoyer",
        "159": "Luke Zettlemoyer",
        "160": "Luke Zettlemoyer",
        "161": "Luke Zettlemoyer",
        "162": "Luke Zettlemoyer",
        "163": "Luke Zettlemoyer",
        "164": "Luke Zettlemoyer",
        "165": "Luke Zettlemoyer",
        "166": "Luke Zettlemoyer",
        "167": "Luke Zettlemoyer",
        "168": "Luke Zettlemoyer",
        "169": "Luke Zettlemoyer",
        "170": "Luke Zettlemoyer",
        "171": "Luke Zettlemoyer",
        "172": "Luke Zettlemoyer",
        "173": "Luke Zettlemoyer",
        "174": "Luke Zettlemoyer",
        "175": "Luke Zettlemoyer",
        "176": "Luke Zettlemoyer",
        "177": "Luke Zettlemoyer",
        "178": "Luke Zettlemoyer",
        "179": "Luke Zettlemoyer",
        "180": "Luke Zettlemoyer",
        "181": "Luke Zettlemoyer",
        "182": "Luke Zettlemoyer",
        "183": "Luke Zettlemoyer",
        "184": "Luke Zettlemoyer",
        "185": "Luke Zettlemoyer",
        "186": "Luke Zettlemoyer",
        "187": "Luke Zettlemoyer",
        "188": "Luke Zettlemoyer",
        "189": "Luke Zettlemoyer",
        "190": "Luke Zettlemoyer",
        "191": "Luke Zettlemoyer",
        "192": "Luke Zettlemoyer",
        "193": "Luke Zettlemoyer",
        "194": "Luke Zettlemoyer",
        "195": "Luke Zettlemoyer",
        "196": "Luke Zettlemoyer",
        "197": "Luke Zettlemoyer",
        "198": "Luke Zettlemoyer",
        "199": "Luke Zettlemoyer",
        "200": "Luke Zettlemoyer",
        "201": "Luke Zettlemoyer",
        "202": "Luke Zettlemoyer",
        "203": "Luke Zettlemoyer",
        "204": "Luke Zettlemoyer",
        "205": "Luke Zettlemoyer",
        "206": "Luke Zettlemoyer",
        "207": "Luke Zettlemoyer",
        "208": "Luke Zettlemoyer",
        "209": "Luke Zettlemoyer",
        "210": "Luke Zettlemoyer",
        "211": "Luke Zettlemoyer",
        "212": "Luke Zettlemoyer",
        "213": "Luke Zettlemoyer",
        "214": "Luke Zettlemoyer",
        "215": "Luke Zettlemoyer",
        "216": "Luke Zettlemoyer",
        "217": "Luke Zettlemoyer",
        "218": "Luke Zettlemoyer",
        "219": "Luke Zettlemoyer",
        "220": "Luke Zettlemoyer",
        "221": "Luke Zettlemoyer",
        "222": "Luke Zettlemoyer",
        "223": "Luke Zettlemoyer",
        "224": "Luke Zettlemoyer",
        "225": "Luke Zettlemoyer",
        "226": "Luke Zettlemoyer",
        "227": "Luke Zettlemoyer",
        "228": "Luke Zettlemoyer",
        "229": "Luke Zettlemoyer",
        "230": "Luke Zettlemoyer",
        "231": "Luke Zettlemoyer",
        "232": "Luke Zettlemoyer",
        "233": "Luke Zettlemoyer",
        "234": "Luke Zettlemoyer",
        "235": "Luke Zettlemoyer",
        "236": "Luke Zettlemoyer",
        "237": "Luke Zettlemoyer",
        "238": "Luke Zettlemoyer",
        "239": "Luke Zettlemoyer",
        "240": "Luke Zettlemoyer",
        "241": "Luke Zettlemoyer",
        "242": "Luke Zettlemoyer",
        "243": "Luke Zettlemoyer",
        "244": "Luke Zettlemoyer",
        "245": "Luke Zettlemoyer",
        "246": "Luke Zettlemoyer",
        "247": "Luke Zettlemoyer",
        "248": "Luke Zettlemoyer",
        "249": "Luke Zettlemoyer",
        "250": "Luke Zettlemoyer",
        "251": "Luke Zettlemoyer",
        "252": "Luke Zettlemoyer",
        "253": "Luke Zettlemoyer",
        "254": "Luke Zettlemoyer",
        "255": "Luke Zettlemoyer",
        "256": "Luke Zettlemoyer",
        "257": "Luke Zettlemoyer",
        "258": "Luke Zettlemoyer",
        "259": "Luke Zettlemoyer",
        "260": "Luke Zettlemoyer",
        "261": "Luke Zettlemoyer",
        "262": "Luke Zettlemoyer",
        "263": "Luke Zettlemoyer"
    },
    "reference_count": {
        "0": 45,
        "1": 40,
        "2": 41,
        "3": 79,
        "4": 72,
        "5": 52,
        "6": 81,
        "7": 45,
        "8": 62,
        "9": 46,
        "10": 62,
        "11": 25,
        "12": 30,
        "13": 34,
        "14": 45,
        "15": 69,
        "16": 61,
        "17": 30,
        "18": 149,
        "19": 33,
        "20": 50,
        "21": 61,
        "22": 40,
        "23": 78,
        "24": 84,
        "25": 41,
        "26": 49,
        "27": 39,
        "28": 27,
        "29": 96,
        "30": 55,
        "31": 46,
        "32": 120,
        "33": 40,
        "34": 54,
        "35": 31,
        "36": 52,
        "37": 29,
        "38": 72,
        "39": 82,
        "40": 43,
        "41": 81,
        "42": 54,
        "43": 121,
        "44": 40,
        "45": 81,
        "46": 49,
        "47": 50,
        "48": 66,
        "49": 52,
        "50": 101,
        "51": 80,
        "52": 72,
        "53": 20,
        "54": 57,
        "55": 72,
        "56": 34,
        "57": 98,
        "58": 82,
        "59": 76,
        "60": 48,
        "61": 45,
        "62": 26,
        "63": 96,
        "64": 53,
        "65": 34,
        "66": 25,
        "67": 24,
        "68": 7,
        "69": 67,
        "70": 43,
        "71": 40,
        "72": 30,
        "73": 67,
        "74": 81,
        "75": 46,
        "76": 41,
        "77": 44,
        "78": 53,
        "79": 26,
        "80": 64,
        "81": 73,
        "82": 137,
        "83": 65,
        "84": 49,
        "85": 77,
        "86": 75,
        "87": 84,
        "88": 65,
        "89": 59,
        "90": 32,
        "91": 70,
        "92": 49,
        "93": 53,
        "94": 42,
        "95": 67,
        "96": 55,
        "97": 91,
        "98": 63,
        "99": 43,
        "100": 44,
        "101": 39,
        "102": 24,
        "103": 32,
        "104": 19,
        "105": 67,
        "106": 52,
        "107": 15,
        "108": 51,
        "109": 52,
        "110": 36,
        "111": 38,
        "112": 32,
        "113": 42,
        "114": 34,
        "115": 33,
        "116": 14,
        "117": 56,
        "118": 23,
        "119": 28,
        "120": 42,
        "121": 14,
        "122": 41,
        "123": 19,
        "124": 51,
        "125": 46,
        "126": 39,
        "127": 36,
        "128": 23,
        "129": 19,
        "130": 27,
        "131": 44,
        "132": 25,
        "133": 42,
        "134": 28,
        "135": 33,
        "136": 31,
        "137": 57,
        "138": 23,
        "139": 19,
        "140": 41,
        "141": 8,
        "142": 38,
        "143": 25,
        "144": 26,
        "145": 59,
        "146": 59,
        "147": 34,
        "148": 18,
        "149": 52,
        "150": 0,
        "151": 31,
        "152": 32,
        "153": 63,
        "154": 38,
        "155": 49,
        "156": 31,
        "157": 48,
        "158": 31,
        "159": 32,
        "160": 32,
        "161": 36,
        "162": 56,
        "163": 66,
        "164": 34,
        "165": 17,
        "166": 19,
        "167": 32,
        "168": 37,
        "169": 27,
        "170": 31,
        "171": 25,
        "172": 38,
        "173": 31,
        "174": 37,
        "175": 22,
        "176": 48,
        "177": 0,
        "178": 30,
        "179": 45,
        "180": 39,
        "181": 65,
        "182": 30,
        "183": 54,
        "184": 32,
        "185": 13,
        "186": 21,
        "187": 39,
        "188": 42,
        "189": 34,
        "190": 7,
        "191": 17,
        "192": 57,
        "193": 39,
        "194": 37,
        "195": 27,
        "196": 23,
        "197": 9,
        "198": 36,
        "199": 34,
        "200": 42,
        "201": 38,
        "202": 44,
        "203": 7,
        "204": 33,
        "205": 20,
        "206": 14,
        "207": 54,
        "208": 32,
        "209": 29,
        "210": 36,
        "211": 37,
        "212": 17,
        "213": 40,
        "214": 41,
        "215": 40,
        "216": 31,
        "217": 27,
        "218": 42,
        "219": 30,
        "220": 55,
        "221": 6,
        "222": 28,
        "223": 34,
        "224": 166,
        "225": 28,
        "226": 22,
        "227": 24,
        "228": 24,
        "229": 45,
        "230": 14,
        "231": 64,
        "232": 28,
        "233": 39,
        "234": 57,
        "235": 22,
        "236": 27,
        "237": 0,
        "238": 27,
        "239": 26,
        "240": 22,
        "241": 11,
        "242": 20,
        "243": 36,
        "244": 6,
        "245": 12,
        "246": 11,
        "247": 14,
        "248": 36,
        "249": 23,
        "250": 17,
        "251": 9,
        "252": 31,
        "253": 49,
        "254": 37,
        "255": 7,
        "256": 25,
        "257": 32,
        "258": 24,
        "259": 22,
        "260": 21,
        "261": 18,
        "262": 27,
        "263": 7
    },
    "citation_count": {
        "0": 96,
        "1": 39,
        "2": 14,
        "3": 6,
        "4": 160,
        "5": 0,
        "6": 3,
        "7": 11,
        "8": 6,
        "9": 28,
        "10": 333,
        "11": 109,
        "12": 10,
        "13": 11,
        "14": 1,
        "15": 1,
        "16": 6,
        "17": 0,
        "18": 1,
        "19": 6,
        "20": 0,
        "21": 29,
        "22": 7,
        "23": 29,
        "24": 1,
        "25": 4,
        "26": 10,
        "27": 51,
        "28": 14,
        "29": 25,
        "30": 7,
        "31": 9,
        "32": 1222,
        "33": 48,
        "34": 35,
        "35": 5,
        "36": 44,
        "37": 16,
        "38": 4,
        "39": 182,
        "40": 43,
        "41": 227,
        "42": 42,
        "43": 170,
        "44": 19,
        "45": 6,
        "46": 13,
        "47": 40,
        "48": 43,
        "49": 18,
        "50": 43,
        "51": 45,
        "52": 26,
        "53": 7,
        "54": 74,
        "55": 413,
        "56": 39,
        "57": 67,
        "58": 47,
        "59": 2,
        "60": 15,
        "61": 6,
        "62": 18,
        "63": 19,
        "64": 6,
        "65": 12,
        "66": 11,
        "67": 9,
        "68": 0,
        "69": 89,
        "70": 37,
        "71": 1,
        "72": 15,
        "73": 73,
        "74": 32,
        "75": 18,
        "76": 72,
        "77": 1,
        "78": 35,
        "79": 11,
        "80": 10,
        "81": 91,
        "82": 190,
        "83": 6,
        "84": 39,
        "85": 21,
        "86": 190,
        "87": 69,
        "88": 112,
        "89": 68,
        "90": 128,
        "91": 3,
        "92": 23,
        "93": 3,
        "94": 50,
        "95": 120,
        "96": 65,
        "97": 85,
        "98": 93,
        "99": 18,
        "100": 179,
        "101": 144,
        "102": 25,
        "103": 88,
        "104": 22,
        "105": 1084,
        "106": 43,
        "107": 56,
        "108": 118,
        "109": 24,
        "110": 146,
        "111": 128,
        "112": 67,
        "113": 67,
        "114": 155,
        "115": 61,
        "116": 37,
        "117": 15065,
        "118": 265,
        "119": 265,
        "120": 83,
        "121": 34,
        "122": 217,
        "123": 7,
        "124": 194,
        "125": 140,
        "126": 10,
        "127": 6203,
        "128": 330,
        "129": 412,
        "130": 158,
        "131": 249,
        "132": 36,
        "133": 3743,
        "134": 19,
        "135": 26,
        "136": 427,
        "137": 1465,
        "138": 15,
        "139": 127,
        "140": 180,
        "141": 3,
        "142": 43,
        "143": 29,
        "144": 174,
        "145": 359,
        "146": 435,
        "147": 50,
        "148": 71,
        "149": 559,
        "150": 13,
        "151": 612,
        "152": 78,
        "153": 10063,
        "154": 162,
        "155": 118,
        "156": 30,
        "157": 47,
        "158": 243,
        "159": 176,
        "160": 1115,
        "161": 100,
        "162": 328,
        "163": 94,
        "164": 25,
        "165": 398,
        "166": 100,
        "167": 68,
        "168": 309,
        "169": 765,
        "170": 417,
        "171": 26,
        "172": 280,
        "173": 34,
        "174": 1210,
        "175": 450,
        "176": 545,
        "177": 0,
        "178": 209,
        "179": 36,
        "180": 83,
        "181": 25,
        "182": 37,
        "183": 222,
        "184": 23,
        "185": 28,
        "186": 66,
        "187": 56,
        "188": 57,
        "189": 191,
        "190": 0,
        "191": 66,
        "192": 155,
        "193": 12,
        "194": 96,
        "195": 37,
        "196": 2,
        "197": 2,
        "198": 304,
        "199": 136,
        "200": 79,
        "201": 55,
        "202": 34,
        "203": 0,
        "204": 398,
        "205": 12,
        "206": 29,
        "207": 33,
        "208": 104,
        "209": 22,
        "210": 4,
        "211": 12,
        "212": 38,
        "213": 1,
        "214": 321,
        "215": 34,
        "216": 358,
        "217": 1,
        "218": 73,
        "219": 82,
        "220": 456,
        "221": 0,
        "222": 97,
        "223": 392,
        "224": 3,
        "225": 310,
        "226": 3,
        "227": 21,
        "228": 48,
        "229": 83,
        "230": 2,
        "231": 21,
        "232": 2,
        "233": 222,
        "234": 145,
        "235": 965,
        "236": 121,
        "237": 1,
        "238": 320,
        "239": 201,
        "240": 294,
        "241": 30,
        "242": 158,
        "243": 460,
        "244": 4,
        "245": 11,
        "246": 22,
        "247": 14,
        "248": 221,
        "249": 927,
        "250": 81,
        "251": 6,
        "252": 80,
        "253": 17,
        "254": 0,
        "255": 35,
        "256": 0,
        "257": 106,
        "258": 42,
        "259": 46,
        "260": 54,
        "261": 42,
        "262": 35,
        "263": 32
    },
    "tldr": {
        "0": "",
        "1": "Automatic Reasoning and Tool-use (ART), a framework that uses frozen<br>LLMs to automatically generate intermediate reasoning steps as a program,<br>achieves a substantial improvement over few-shot prompting and automatic CoT<br>on unseen tasks in the BigBench and MMLU benchmarks, and<br>matches performance of hand-crafted CoT prompts on a majority of<br>these tasks.",
        "2": "CM3Leon is a retrieval-augmented, token-based, decoder-only multi-modal language model capable<br>of generating and infilling both text and images and introduces<br>self-contained contrastive decoding methods that produce high-quality outputs.",
        "3": "SwitchBack is introduced, a linear layer for int8 quantized training<br>which provides a speed-up of 13-25% while matching the performance<br>of bfloat16 training within 0.1 percentage points for the 1B<br>parameter CLIP ViT-Huge -- the largest int8 training to date.",
        "4": "QLoRA finetuning on a small high-quality dataset leads to state-of-the-art<br>results, even when using smaller models than the previous SoTA,<br>and current chatbot benchmarks are not trustworthy to accurately evaluate<br>the performance levels of chatbots.",
        "5": "This work proposes a new layer named Gated State Space<br>(GSS) and shows that it trains significantly faster than the<br>diagonal version of S4 on TPUs, is competitive with several<br>well-tuned Transformer-based baselines and exhibits zero-shot generalization to longer inputs<br>while being straightforward to implement.",
        "6": "It is found that despite significant cross-lingual transfer in English-centric<br>LLMs, much smaller MLMs pretrained on balanced multilingual data still<br>understand far more languages and that larger vocabulary size and<br>conscious vocabulary construction correlate with better performance on low-resource languages.",
        "7": "Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling<br>of sequences of over one million bytes, is proposed, establishing<br>the viability of tokenization-free autoregressive sequence modeling at scale.",
        "8": "This work introduces a simple but effective method to asynchronously<br>train large, sparse language models on arbitrary text corpora, which<br>clusters a corpus into sets of related documents, trains a<br>separate expert language model on each cluster, and combines them<br>in a sparse ensemble for inference.",
        "9": "New mixed-modal scaling laws are reported that unify the contributions<br>of individual modalities and the interactions between them and are<br>tested by training a 30B speech-text model, which significantly outperforms<br>the corresponding unimodal models.",
        "10": "This paper introduces Toolformer, a model trained to decide which<br>APIs to call, when to call them, what arguments to<br>pass, and how to best incorporate the results into future<br>token prediction, which achieves substantially improved zero-shot performance across a<br>variety of downstream tasks.",
        "11": "Training LIMA strongly suggests that almost all knowledge in large<br>language models is learned during pretraining, and only limited instruction<br>tuning data is necessary to teach models to produce high<br>quality output.",
        "12": "It is shown that LLM prompting can provide an effective<br>solution for rare words as well, by using prior knowledge<br>from bilingual dictionaries to provide control hints in the prompts,<br>thereby enabling fine-grained phrase-level prompted control of the LLM.",
        "13": "A new approach for scaling to very large multilingual vocabularies<br>is introduced by de-emphasizing token sharing between languages with little<br>lexical overlap and assigning vocabulary capacity to achieve sufficient coverage<br>for each individual language.",
        "14": "A Mixture-of-Prompt-Experts (MOPE) system that ensembles multiple specialized LLMs that<br>significantly outperforms any single specialized model on a collection of<br>12 QA datasets from four reasoning types.",
        "15": "It is demonstrated empirically and theoretically that MLM pretraining allocates<br>some model dimensions exclusively for representing real tokens, resulting in<br>a representation deficiency for real tokens and limiting the pretrained<br>model's expressiveness when it is adapted to downstream data without<br>$\\texttt{[MASK]}$ tokens.",
        "16": "This work introduces Shepherd, a language model specifically tuned to<br>critique responses and suggest refinements, extending beyond the capabilities of<br>an untuned model to identify diverse errors and provide suggestions<br>to remedy them.",
        "17": "A zero-shot approach for WSD, tested on 18 languages from<br>the XL-WSD dataset, and finds that as the model size<br>increases, PLMs encode more cross-lingual word sense knowledge and better<br>use context to improve WLT performance.",
        "18": "",
        "19": "Context-aware decoding (CAD), which follows a contrastive output distribution that<br>amplifies the difference between the output probabilities when a model<br>is used with and without context, improves the faithfulness of<br>different LM families.",
        "20": "The Gender-GAP Pipeline (for Gender-Aware Polyglot Pipeline), an automatic pipeline<br>to characterize gender representation in large-scale datasets for 55 languages,<br>using a multilingual lexicon of gendered person-nouns to quantify the<br>gender represented in text is described.",
        "21": "Retrieval-Augmented CM3 is the first multimodal model that can retrieve<br>and generate mixtures of text and images and exhibits novel<br>capabilities such as knowledge-intensive image generation and multi-modal in-context learning.",
        "22": "Curation in Training is presented, a simple and efficient vision-text<br>learning algorithm that couples a data objective into training and<br>can speed up training by over an order of magnitude,<br>especially if the raw data size is large.",
        "23": "An automated model is introduced that estimates FACTSCORE using retrieval<br>and a strong language model, and is used to evaluate<br>6,500 generations from a new set of 13 recent LMs<br>that would have cost $26K if evaluated by humans, with<br>various findings.",
        "24": "The results suggest that it is possible to build high<br>quality language models while mitigating their legal risk, and which<br>nonparametric approach works best, where the remaining errors lie, and<br>how performance scales with datastore size.",
        "25": "It is shown that, by using a stronger MT system<br>and mitigating the mismatch between training on original text and<br>running inference on machine translated text, translate-test can do substantially<br>better than previously assumed.",
        "26": "This work presents a scalable method to build a high<br>quality instruction following language model by automatically labelling human-written text<br>with corresponding instructions, not relying on distillation data, demonstrating highly<br>effective self-alignment.",
        "27": "",
        "28": "Z-ICL is introduced, a new zero-shot method that closes the<br>gap by constructing pseudo-demonstrations for a given test input using<br>a raw text corpus, and is on par with in-context<br>learning with labeled training data in the few-shot setting.",
        "29": "It is argued that privileging any corpus as high quality<br>entails a language ideology, and more care is needed to<br>construct training corpora for language models, with better transparency and<br>justification for the inclusion or exclusion of various texts.",
        "30": "It is found that while PLMs contain significant prior knowledge<br>of task labels due to task leakage into the pretraining<br>corpus, structured prompting can also retrieve linguistic structure with arbitrary<br>labels, indicating that the in-context learning ability and linguistic knowledge<br>of PLMs generalizes beyond memorization of their training data.",
        "31": "This work investigates when multilingual pretraining models acquire their in-language<br>and cross-lingual abilities by probing checkpoints taken from throughout XLM-R<br>pretraining, using a suite of linguistic tasks.",
        "32": "Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers<br>ranging from 125M to 175B parameters, is presented, which is<br>aimed to fully and responsibly share with interested researchers.",
        "33": "It is shown that CoT reasoning is possible even with<br>invalid demonstrations - prompting with invalid reasoning steps can achieve<br>over 80-90% of the performance obtained using CoT under various<br>metrics, while still generating coherent lines of reasoning during inference.",
        "34": "ROSCOE is presented, a suite of interpretable, unsupervised automatic scores<br>that improve and extend previous text generation evaluation metrics and<br>can measure semantic consistency, logicality, informativeness, fluency, and factuality -<br>among other traits - by leveraging properties of step-by-step rationales.",
        "35": "This work proposes Language Dynamics Distillation (LDD), which pretrains a<br>model to predict environment dynamics given demonstrations with language descriptions,<br>and then fine-tunes these language-aware pretrained representations via reinforcement learning<br>(RL).",
        "36": "This work introduces execution result\u2013based minimum Bayes risk decoding (MBR-EXEC)<br>for program selection and finds that it consistently improves over<br>all execution-unaware selection methods, suggesting it as an effective approach<br>for natural language to code translation.",
        "37": "",
        "38": "Experiments on natural language inference and sentiment analysis benchmarks show<br>that CORE counterfactuals are more effective at improving generalization to<br>OOD data compared to other DA approaches, and the CORE<br>retrieval framework can be used to encourage diversity in manually<br>authored perturbations.",
        "39": "A procedure for Int8 matrix multiplication for feed-forward and attention<br>projection layers in transformers, which cut the memory needed for<br>inference by half while retaining full precision performance, and makes<br>such models much more accessible.",
        "40": "It is shown that the translation quality and the domain<br>of the in-context examples matter and that 1-shot noisy unrelated<br>example can have a catastrophic impact on output quality.",
        "41": "InCoder is introduced, a unified generative model that can perform<br>program synthesis (via left-to-right generation) as well as editing (via<br>infilling) with the ability to condition on bidirectional context substantially<br>improves performance on these tasks, while still performing comparably on<br>standard program synthesis benchmarks.",
        "42": "The findings show that {4-bit} precision is almost universally optimal<br>for total model bits and zero-shot accuracy, and it is<br>challenging to improve the bit-level scaling trade-off.",
        "43": "The UnifiedSKG framework is proposed, which unifies 21 SKG tasks<br>into a text-to-text format, aiming to promote systematic SKG research,<br>instead of being exclusive to a single task, domain, or<br>dataset.",
        "44": "KNN-Prompt is introduced, a simple and effective kNN-LM with automatically<br>expanded fuzzy verbalizers that is effective for domain adaptation with<br>no further training, and gains increase with the size of<br>the retrieval model.",
        "45": "This work describes LegoNN, a procedure for building encoder-decoder architectures<br>in a way so that its parts can be applied<br>to other tasks without the need for any fine-tuning, and<br>introduces a modality agnostic encoder which consists of a length<br>control mechanism to dynamically adapt encoders' output lengths in order<br>to match the expected input length range of pre-trained decoders.",
        "46": "Analysis of intermediate training checkpoints of differently sized OPT models<br>shows that perplexity is more predictive of model behaviors than<br>model size or training computation.",
        "47": "This work proposes a simple and effective re-ranking method for<br>improving passage retrieval in open question answering that improves strong<br>unsupervised retrieval models by 6%-18% absolute and strong supervised models<br>by up to 12% in terms of top-20 passage retrieval<br>accuracy.",
        "48": "Experiments demonstrate that Perfect, a simple and efficient method for<br>few-shot fine-tuning of PLMs without relying on any such handcrafting,<br>also outperforms existing state-of-the-art few- shot learning methods.",
        "49": "Art, a new corpus-level autoencoding approach for training dense retrieval<br>models that does not require any labeled training data, is<br>introduced, demonstrating state-of-the-art results on multiple QA retrieval benchmarks with<br>only generic initialization from a pre-trained language model.",
        "50": "INSTRUCTOR is a single embedder that can generate text embeddings<br>tailored to different downstream tasks and domains, without any further<br>training, and achieves state-of-the-art performance.",
        "51": "This paper introduces Mega, a simple, theoretically grounded, single-head gated<br>attention mechanism equipped with (exponential) moving average to incorporate inductive<br>bias of position-aware local dependencies into the position-agnostic attention mechanism.",
        "52": "This paper presents a detailed empirical study of how autoregressive<br>MoE language models scale in comparison with dense models in<br>a wide range of settings: in- and out-of-domain language modeling,<br>zero- and few-shot priming, and full-shot fine-tuning.",
        "53": "To improve in-domain performance, the benefits of adapting the LM<br>along a domain hierarchy are shown; adapting to smaller amounts<br>of fine-grained domain-specific data can lead to larger in- domain<br>performance gains than larger amounts of weakly relevant data.",
        "54": "This work forms an annotation-efficient, two-step framework: selective annotation that<br>chooses a pool of examples to annotate from unlabeled data<br>in advance, followed by prompt retrieval that retrieves task examples<br>from the annotated pool at test time, and proposes an<br>unsupervised, graph-based selective annotation method, voke-k, to select diverse, representative<br>examples to annotation.",
        "55": "This paper shows that ground truth demonstrations are in fact<br>not required and that other aspects of the demonstrations are<br>the key drivers of end task performance, including the fact<br>that they provide a few examples of the label space,<br>the distribution of the input text, and the overall format<br>of the sequence.",
        "56": "DS-1000, a code generation benchmark with a thousand data science<br>problems spanning seven Python libraries, such as NumPy and Pandas,<br>is introduced; the current best public system (Codex-002) achieves 43.3%<br>accuracy, leaving ample room for improvement.",
        "57": "It is shown that larger models can memorize a larger<br>portion of the data before over-fitting and tend to forget<br>less throughout the training process, and another piece of the<br>broader puzzle of trying to understand what actually improves as<br>models get bigger is presented.",
        "58": "BTM improves in- and out-of-domain perplexities as compared to GPT-style<br>Transformer LMs, and gains grow with the number of domains,<br>suggesting more aggressive parallelism could be used to efficiently train<br>larger models in future work.",
        "59": "AGRO -- Adversarial Group discovery for Distributionally Robust Optimization --<br>is an end-to-end approach that jointly identifies error-prone groups and<br>improves accuracy on them and equips G-DRO with an adversarial<br>slicing model to find a group assignment for training examples<br>which maximizes worst-case loss over the discovered groups.",
        "60": "This work focuses on bidirectionality as a key factor that<br>differentiates existing approaches, and presents a comprehensive study of its<br>role in next token prediction, text infilling, zero-shot priming and<br>fine-tuning, and proposes a new framework that generalizes prior approaches.",
        "61": "This work introduces the methodology of Faithfulness-through-Counterfactuals, which first generates<br>acounterfactual hypothesis based on the logical predicates expressed in the<br>explanation, and then evaluates if the model's prediction on the<br>counterfactual is consistent with that expressed logic.",
        "62": "The Spoken Task-Oriented semantic Parsing (STOP) dataset 1 is released,<br>the largest and most complex SLU dataset publicly available and<br>low-resource splits are defined to establish a benchmark for improving<br>SLU when limited labeled data is available.",
        "63": "It is shown that NPM can be efficiently trained with<br>a contrastive objective and an in-batch approximation to full corpus<br>retrieval and outperforms significantly larger parametric models, either with or<br>without a retrieve-and-generate approach.",
        "64": "",
        "65": "This paper proposes a human readable prompt tuning method (F<br>LUENT P ROMPT) based on Langevin dynamics that incorporates a<br>fluency constraint to find a diverse distribution of effective and<br>fluent prompts and investigates common attributes shared by effective prompts.",
        "66": "",
        "67": "RoMQA is challenging: zero-shot and few-shot models perform similarly to<br>naive baselines, while supervised retrieval methods perform well below gold<br>evidence upper bounds, and a quantifiable test to build more<br>robust QA methods is provided.",
        "68": "This paper ages recent advances in Bayesian deep learning (BDL)<br>to more accurately identify the best solution from a few<br>rounds of interaction to community question answer (cQA) tasks, finding<br>that the BDL approach significantly outperforms existing methods while remaining<br>robust to noise in the user feedback.",
        "69": "CM3, a family of causally masked generative models trained over<br>a large corpus of structured multi-modal documents that can contain<br>both text and image tokens, is introduced and set the<br>new state-of-the-art in zero-shot summarization, entity linking, and entity disambiguation<br>while maintaining competitive performance in the fine-tuning setting.",
        "70": "Over a wide range of tasks, it is shown that<br>the lower the perplexity of the prompt is, the better<br>the prompts are able to perform the task and a<br>new empirical hypothesis is established: the performance of a prompt<br>is coupled with the extent to which the model is<br>familiar with the language it contains.",
        "71": "This work mines naturally occurring high-quality input output pairs to<br>mimic the style of the seed set for multiple tasks,<br>and sees improvements of 1.46 ROUGE-L on Xsum abstractive summarization.",
        "72": "This paper introduces FEWS (Few-shot Examples of Word Senses), a<br>new low-shot WSD dataset automatically extracted from example sentences in<br>Wiktionary that provides a large training set that covers many<br>more senses than previous datasets and a comprehensive evaluation set<br>containing few- and zero-shot examples of a wide variety of<br>senses.",
        "73": "This paper develops a fast, high-precision non-linear quantization method \u2013<br>block-wise dynamic quantization \u2013 that enables stable 8-bit optimizers which<br>maintain 32-bit performance at a fraction of the memory footprint<br>and without any changes to the original hyperparameters.",
        "74": "A detailed analysis of where the model succeeds and fails<br>is presented, showing in particular that it enables cross-lingual in-context<br>learning on some tasks, while there is still room for<br>improvement on surface form robustness and adaptation to tasks that<br>do not have a natural cloze form.",
        "75": "This paper constructs a large-scale challenging fact verification dataset called<br>FAVIQ, consisting of 188k claims derived from an existing corpus<br>of ambiguous information-seeking questions, verified to be natural, contain little<br>lexical bias, and require a complete understanding of the evidence<br>for verification.",
        "76": "A simplified, task-agnostic multi-modal pre-training approach that can accept either<br>video or text input, or both for a variety of<br>end tasks, and introduces new pretraining masking schemes that better<br>mix across modalities.",
        "77": "Compared to other multi-document summarization tasks, this task is entity-centric,<br>more abstractive, and covers a wide range of domains, and<br>there exists a large gap between state-of-art models and human<br>performance.",
        "78": "A novel reasoning task that targets both visual and non-visual<br>language about 3D objects, and finds that adding view estimation<br>to language grounding models improves accuracy on both SNARE and<br>when identifying objects referred to in language on a robot<br>platform, but note that a large gap remains between these<br>models and human performance.",
        "79": "Low-rank adaptive label smoothing (LORAS) is proposed, a simple yet<br>novel method for training with learned soft targets that generalizes<br>label smoothed and adapts to the latent structure of the<br>label space in structured prediction tasks.",
        "80": "A large-scale empirical study of the features and limits of<br>LM adaptability using a new benchmark, TaskBench500, built from 500<br>procedurally generated sequence modeling tasks, shows that adaptability to new<br>tasks, like generalization to new examples, can be systematically described<br>and understood.",
        "81": "This work trains multilingual generative language models on a corpus<br>covering a diverse set of languages, and shows in particular<br>that strong few-shot learning performance across languages can be achieved<br>via cross-lingual transfer through both templates and demonstration examples.",
        "82": "This work introduces MetaICL (Meta-training for In-Context Learning), a new<br>meta-training framework for few-shot learning where a pretrained language model<br>is tuned to do in-context learning on a large set<br>of training tasks.",
        "83": "The multi-environment Symbolic Interactive Language Grounding benchmark (SILG) is proposed,<br>which unifies a collection of diverse grounded language learning environments<br>under a common interface and enables the community to quickly<br>identify new methodologies for language grounding that generalize to a<br>diverse set of environments and their associated challenges.",
        "84": "Inspired by the contrastive nature of human explanations, this work<br>uses PLMs to complete explanation prompts which contrast alternatives according<br>to the key attribute(s) required to justify the correct answer<br>(for example, peanuts are usually salty while raisins are sweet).",
        "85": "A bi-encoder QA model is trained, which independently encodes passages<br>and questions, to match the predictions of a more accurate<br>cross-encoding model on 80 million synthesized QA pairs, and shows<br>large improvements over both RoBERTa-large and previous state-of-the-art results on<br>zero-shot and few-shot paraphrase detection on four datasets.",
        "86": "It is shown that pre-finetuning consistently improves performance for pretrained<br>discriminators and generation models on a wide range of tasks<br>while also significantly improving sample efficiency during fine-tuning, and that<br>large-scale multi-tasking is crucial.",
        "87": "A new domain expert mixture (DEMix) layer that enables conditioning<br>a language model (LM) on the domain of the input<br>text, and shows it is possible to add experts to<br>adapt to new domains without forgetting older ones, and remove<br>experts to restrict access to unwanted domains.",
        "88": "Domain Conditional Pointwise Mutual Information is introduced, an alternative scoring<br>function that directly compensates for surface form competition by simply<br>reweighing each option according to its a priori likelihood within<br>the context of a specific task.",
        "89": "Luna is proposed, a linear unified nested attention mechanism that<br>approximates softmax attention with two nested linear attention functions, yielding<br>only linear time and space complexity.",
        "90": "A new balanced assignment of experts (BASE) layer for large<br>language models that greatly simplifies existing high capacity sparse layers<br>and improves efficiency by guaranteeing balanced compute loads, and also<br>simplifies training by not requiring any new hyperparameters or auxiliary<br>losses.",
        "91": "This work uses a simple editing strategy by mining potentially<br>imperfect translations for each sentence in a given bitext, and<br>learning a model to reconstruct the original translations and translate,<br>in a multi-task fashion to improve the quality of CCMatrix<br>mined bitext.",
        "92": "The final model outperforms the state of the art on<br>the BUCC 2020 shared task by 14 F1 points averaged<br>over 12 language pairs, while also providing a more interpretable<br>approach that allows for rich reasoning of word meaning in<br>context.",
        "93": "It is shown it is possible to automatically induce semantic<br>roles from QA-SRL, a scal-able and ontology-free semantic annotation scheme<br>that uses question-answer pairs to rep-resent predicate-argument structure.",
        "94": "It is found thathyper-text prompts provide more value to HTLM,<br>in terms of data efficiency, than plain text prompts do<br>for existing LMs, and that HTLM is highly effective at<br>auto-prompting itself, by simply generating the most likely hyper-text formatting<br>for any available training data.",
        "95": "A noisy channel approach for language model prompting in few-shot<br>text classification by using channel models for recently proposed few-<br>shot learning methods with no or very limited updates to<br>the language model parameters, via either in-context demonstration or prompt<br>tuning.",
        "96": "",
        "97": "This paper presents a detailed empirical study of how autoregressive<br>MoE language models scale in comparison with dense models in<br>a wide range of settings: in- and out-of-domain language modeling,<br>zero- and few-shot priming, and full-shot fine-tuning.",
        "98": "A new task to predict whether each token in the<br>output sequence is hallucinated conditioned on the source input, and<br>a novel method for learning to model hallucination detection, based<br>on pretrained language models fine tuned on synthetic data that<br>includes automatically inserted hallucinations.",
        "99": "This work proposes a new semantic scheme for capturing predicate-argument<br>relations for nominalizations, termed QANom, which extends the QA-SRL formalism,<br>modeling the relations between nominalizations and their arguments via natural<br>language question-answer pairs.",
        "100": "This work introduces $k$-nearest-neighbor machine translation ($k$NN-MT), which predicts tokens<br>with a nearest neighbor classifier over a large datastore of<br>cached examples, using representations from a neural translation model for<br>similarity search.",
        "101": "This paper introduces AmbigQA, a new open-domain question answering task<br>which involves predicting a set of question-answer pairs, where every<br>plausible answer is paired with a disambiguated rewrite of the<br>original question.",
        "102": "This work uses a standard editing model with simple task-specific<br>re-ranking approaches, and shows empirically that this approach outperforms existing,<br>significantly more complex methodologies.",
        "103": "Aligned cross entropy (AXE) as an alternative loss function for<br>training of non-autoregressive models and AXE-based training of conditional masked<br>language models (CMLMs) substantially improves performance on major WMT benchmarks,<br>while setting a new state of the art for non-AUTOgressive<br>models.",
        "104": "This work improves upon pairwise annotation for active learning in<br>coreference resolution, by asking annotators to identify mention antecedents if<br>a presented mention pair is deemed not coreferent.",
        "105": "",
        "106": "This paper proposes a method that can automatically detect and<br>ignore dataset-specific patterns, which it hypothesize are likely to reflect<br>dataset bias, and trains a lower capacity model in an<br>ensemble with a higher capacity model.",
        "107": "A new training method is introduced for conditional masked language<br>models, SMART, which mimics the semi-autoregressive behavior of mask-predict, producing<br>training examples that contain model predictions as part of their<br>inputs.",
        "108": "This work proposes a bi-encoder model that independently embeds the<br>target word with its surrounding context and the dictionary definition,<br>or gloss, of each sense, and demonstrates that rare senses<br>can be more effectively disambiguated by modeling their definitions.",
        "109": "A very deep and light-weight transformer, DeLighT, that delivers similar<br>or better performance than transformer-based models with significantly fewer parameters,<br>and that matches the performance of baseline Transformers with significantly<br>less parameters.",
        "110": "A simplified and efficient method rooted in trust region theory<br>that replaces previously used adversarial objectives with parametric noise (sampling<br>from either a normal or uniform distribution), thereby discouraging representation<br>change during fine-tuning when possible without hurting performance.",
        "111": "It is shown that fine-tuning gives strong performance on a<br>range of discriminative and generative tasks in many languages, making<br>MARGE the most generally applicable pre-training method to date.",
        "112": "GAZP outperforms data-augmentation in the training environment, performance increases with<br>the amount of GAZP-synthesized data, and cycle-consistency is central to<br>successful adaptation.",
        "113": "This paper shows that it is possible to better manage<br>this trade-off by optimizing a bound on the Information Bottleneck<br>(IB) objective, and derives a learning objective that allows direct<br>control of mask sparsity levels through a tunable sparse prior.",
        "114": "This paper empirically shows that common pre-trained models have a<br>very low intrinsic dimension, and connects intrinsic dimensionality with low<br>dimensional task representations and compression based generalization bounds to provide<br>intrinsic-dimension-based generalizations bounds that are independent of the full parameter<br>count.",
        "115": "This work identifies two fundamental factors for low-resource domain adaptation:<br>better representation learning and better training techniques, and proposes a<br>novel method that outperforms a supervised neural model at a<br>10-fold data reduction.",
        "116": "An improved crowdsourcing protocol for complex semantic annotation, involving worker<br>selection and training, and a data consolidation phase is presented,<br>which yielded high-quality annotation with drastically higher coverage, producing a<br>new gold evaluation dataset.",
        "117": "It is found that BERT was significantly undertrained, and can<br>match or exceed the performance of every model published after<br>it, and the best model achieves state-of-the-art results on GLUE,<br>RACE and SQuAD.",
        "118": "A qualitative analysis of model predictions indicates that, compared to<br>ELMo and Bert-base, BERT-large is particularly better at distinguishing between<br>related but distinct entities, but that there is still room<br>for improvement in modeling document-level context, conversations, and mention paraphrasing.",
        "119": "An automatic gender bias evaluation method for eight target languages<br>with grammatical gender, based on morphological analysis is devised, which<br>shows that four popular industrial MT systems and two recent<br>state-of-the-art academic MT models are significantly prone to gender-biased translation<br>errors for all tested target languages.",
        "120": "This work introduces an approach for open-domain question answering (QA)<br>that retrieves and reads a passage graph, where vertices are<br>passages of text and edges represent relationships that are derived<br>from an external knowledge base or co-occurrence in the same<br>article.",
        "121": "This model improves stateof-the-art performance levels for constant-time translation models<br>by over 3 BLEU on average, and is able to<br>reach 92-95% of the performance of a typical left-to-right transformer<br>model, while decoding significantly faster.",
        "122": "This work introduces Cooperative Vision-and-Dialog Navigation, a dataset of over<br>2k embodied, human-human dialogs situated in simulated, photorealistic home environments<br>and establishes an initial, multi-modal sequence-to-sequence model.",
        "123": "This work incorporates morphological supervision into character language models (CLMs)<br>via multitasking and shows that this addition improves bits-per-character (BPC)<br>performance across 24 languages, even when the morphology data and<br>language modeling data are disjoint.",
        "124": "It is shown that transfer is possible even when there<br>is no shared vocabulary across the monolingual corpora and also<br>when the text comes from very different domains, and it<br>is strongly suggested that, much like for non-contextual word embeddings,<br>there are universal latent symmetries in the learned embedding spaces.",
        "125": "This paper develops a hard EM learning scheme that computes<br>gradients relative to the most likely solution at each update<br>and significantly outperforms previous methods on six QA tasks, including<br>absolute gains of 2\u201310%, and achieves the state-of-the-art on five<br>of them.",
        "126": "This work introduces a method for enforcing encoder-decoder modularity in<br>seq2seq models without sacrificing the overall model quality or its<br>full differentiability, and discretizes the encoder output units into a<br>predefined interpretable vocabulary space using the Connectionist Temporal Classification (CTC)<br>loss.",
        "127": "BART is presented, a denoising autoencoder for pretraining sequence-to-sequence models,<br>which matches the performance of RoBERTa on GLUE and SQuAD,<br>and achieves new state-of-the-art results on a range of abstractive<br>dialogue, question answering, and summarization tasks.",
        "128": "This paper introduces a simple and effective two-stage approach for<br>zero-shot linking, based on fine-tuned BERT architectures, and shows that<br>it performs well in the non-zero-shot setting, obtaining the state-of-the-art<br>result on TACKBP-2010.",
        "129": "This model improves state-of-the-art performance levels for non-autoregressive and parallel<br>decoding translation models by over 4 BLEU on average, and<br>is able to reach within about 1 BLEu point of<br>a typical left-to-right transformer model, while decoding significantly faster.",
        "130": "This paper proposes replacing the sinusoidal positional embedding for transformers<br>with convolutionally learned input representations that provide subsequent transformer blocks<br>with relative positional information needed for discovering long-range relationships between<br>local concepts.",
        "131": "This work develops sparse momentum, an algorithm which uses exponentially<br>smoothed gradients (momentum) to identify layers and weights which reduce<br>the error efficiently and shows that the benefits of momentum<br>redistribution and growth increase with the depth and size of<br>the network.",
        "132": "An iterative method to extract code idioms from large source<br>code corpora by repeatedly collapsing most-frequent depth-2 subtrees of their<br>syntax trees, and train semantic parsers to apply these idioms<br>during decoding.",
        "133": "It is shown that pretraining multilingual language models at scale<br>leads to significant performance gains for a wide range of<br>cross-lingual transfer tasks, and the possibility of multilingual modeling without<br>sacrificing per-language performance is shown for the first time.",
        "134": "A semantic parser for parsing compositional utterances into Task Oriented<br>Parse (TOP), a tree representation that has intents and slots<br>as labels of nesting tree nodes, which outperforms previous methods<br>on the TOP dataset of mixed-domain task-oriented utterances in both<br>accuracy and training speed.",
        "135": "A new conversational machine reading model that jointly extracts a<br>set of decision rules from the procedural text while reasoning<br>about which are entailed by the conversational history and which<br>still need to be edited to create questions for the<br>user is presented.",
        "136": "It is suggested that learning similarity between sequences of text<br>is easier than predicting the next word, and that nearest<br>neighbor search is an effective approach for language modeling in<br>the long tail.",
        "137": "The approach extends BERT by masking contiguous random spans, rather<br>than random tokens, and training the span boundary representations to<br>predict the entire content of the masked span, without relying<br>on the individual token representations within it.",
        "138": "A new architecture for storing and accessing entity mentions during<br>online text processing is presented, and it is possible to<br>train the model end-to-end, using both a supervised anaphora resolution<br>objective as well as a supplementary language modeling objective.",
        "139": "This work introduces a single-hop BERT-based RC model that achieves<br>67 F1\u2014comparable to state-of-the-art multi-hop models and designs an evaluation<br>setting where humans are not shown all of the necessary<br>paragraphs for the intendedmulti-hop reasoning but can still answer over<br>80% of questions.",
        "140": "A new approach for pretraining a bi-directional transformer model that<br>provides significant performance gains across a variety of language understanding<br>problems, including cloze-style word reconstruction task, and a detailed analysis<br>of a number of factors that contribute to effective pretraining.",
        "141": "An improved QA-SRL annotation protocol is presented, involving crowd-worker selection<br>and training, followed by data consolidation, yielding more consistent annotations<br>and greater coverage, which will facilitate future replicable research of<br>natural semantic annotations.",
        "142": "A novel iterative training algorithm is proposed that alternates between<br>searching for consistent logical forms and maximizing the marginal likelihood<br>of the retrieved ones, thus dealing with the problem of<br>spuriousness.",
        "143": "Three different improvements to the semantic parsing model are presented:<br>contextualized embeddings, ensembling, and pairwise re-ranking based on a language<br>model, which gives a new state-of-the-art result on the Task<br>Oriented Parsing (TOP) dataset.",
        "144": "A system that decomposes a compositional question into simpler sub-questions<br>that can be answered by off-the-shelf single-hop RC models is<br>proposed and a new global rescoring approach is introduced that<br>considers each decomposition to select the best final answer, greatly<br>improving overall performance.",
        "145": "This paper trains a naive model that makes predictions exclusively<br>based on dataset biases, and a robust model as part<br>of an ensemble with the naive one in order to<br>encourage it to focus on other patterns in the data<br>that are more likely to generalize.",
        "146": "It is shown that a baseline model based on recent<br>embodied vision-and-language tasks performs poorly on ALFRED, suggesting that there<br>is significant room for developing innovative grounded visual language understanding<br>models with this benchmark.",
        "147": "To study code generation conditioned on a long context history,<br>JuICe is presented, a corpus of 1.5 million examples with<br>a curated test set of 3.7K instances based on online<br>programming assignments that provides refined human-curated data, open-domain code, and<br>an order of magnitude more training data.",
        "148": "This paper presents new evidence that this benchmark can be<br>nearly solved by standard methods, and introduces a baseline that<br>sets a new state-of-the-art performance level at 78.1% accuracy, despite<br>using standard methods.",
        "149": "A combination of automated and human evaluations show that SCPNs<br>generate paraphrases that follow their target specifications without decreasing paraphrase<br>quality when compared to baseline (uncontrolled) paraphrase systems.",
        "150": "This work describes a particularly challenging setting for semantic parsing,<br>where there is additional context or interaction that the parser<br>must take into account when translating natural language to formal<br>language, and gives an overview of recent work in this<br>direction.",
        "151": "QuAC introduces challenges not found in existing machine comprehension datasets:<br>its questions are often more open-ended, unanswerable, or only meaningful<br>within the dialog context, as it shows in a detailed<br>qualitative evaluation.",
        "152": "A new large-scale corpus of Question-Answer driven Semantic Role Labeling<br>(QA-SRL) annotations, and the first high-quality QA- SRL parser are<br>presented, and neural models for two QA -SRL subtasks are<br>presented: detecting argument spans for a predicate and generating questions<br>to label the semantic relationship.",
        "153": "A new type of deep contextualized word representation is introduced<br>that models both complex characteristics of word use and how<br>these uses vary across linguistic contexts, allowing downstream models to<br>mix different types of semi-supervision signals.",
        "154": "A model that can predict ultra-fine types is presented, and<br>is trained using a multitask objective that pools the authors'<br>new head-word supervision with prior supervision from entity linking, and<br>achieves state of the art performance on an existing fine-grained<br>entity typing benchmark, and sets baselines for newly-introduced datasets.",
        "155": "This work presents new data and semantic parsing methods for<br>the problem of mapping English sentences to Bash commands (NL2Bash),<br>and takes a first step in enabling any user to<br>perform operations by simply stating their goals in English.",
        "156": "This work decoupling the LSTM\u2019s gates from the embedded simple<br>RNN, producing a new class of RNNs where the recurrence<br>computes an element-wise weighted sum of context-independent functions of the<br>input.",
        "157": "P pairwise embeddings of word pairs are computed as a<br>compositional function of each word\u2019s representation, which is learned by<br>maximizing the pointwise mutual information (PMI) with the contexts in<br>which the the two words co-occur.",
        "158": "A novel formulation of Open IE as a sequence tagging<br>problem, addressing challenges such as encoding multiple extractions for a<br>predicate, and a supervised model that outperforms the existing state-of-the-art<br>Open IE systems on benchmark datasets.",
        "159": "This work proposes an end-to-end approach for jointly predicting all<br>predicates, arguments spans, and the relations between them, and makes<br>independent decisions about what relationship, if any, holds between every<br>possible word-span pair.",
        "160": "AllenNLP is described, a library for applying deep learning methods<br>to NLP research that addresses issues with easy-to-use command-line tools,<br>declarative configuration-driven experiments, and modular NLP abstractions.",
        "161": "These end-to-end neural models establish a new state-of-the-art on existing<br>verb metaphor detection benchmarks, and show strong performance on jointly<br>predicting the metaphoricity of all words in a running text.",
        "162": "There is a tradeoff between speed and accuracy, but all<br>architectures learn high quality contextual representations that outperform word embeddings<br>for four challenging NLP tasks, suggesting that unsupervised biLMs, independent<br>of architecture, are learning much more about the structure of<br>language than previously appreciated.",
        "163": "This work introduces the syntactic scaffold, an approach to incorporating<br>syntactic information into semantic tasks through a multitask objective, and<br>improves over strong baselines on PropBank semantics, frame semantics, and<br>coreference resolution.",
        "164": "It is found that it is possible for a non-expert<br>adversary to defeat a source code attribution system designed to<br>be adversarially resistant.",
        "165": "This work introduces a fully-differentiable approximation to higher-order inference for<br>coreference resolution that significantly improves accuracy on the English OntoNotes<br>benchmark, while being far more computationally efficient.",
        "166": "A set of experiments is presented to demonstrate that deep<br>recurrent neural networks learn internal representations that capture soft hierarchical<br>notions of syntax from highly varied supervision, indicating that a<br>soft syntactic hierarchy emerges.",
        "167": "A crowdsourcing scheme is developed to show that QAMRs can<br>be labeled with very little training, and a qualitative analysis<br>demonstrates that the crowd-generated question-answer pairs cover the vast majority<br>of predicate-argument relationships in existing datasets.",
        "168": "An approach to rapidly and easily build natural language interfaces<br>to databases for new domains, whose performance improves over time<br>based on user feedback, and requires minimal intervention is presented.",
        "169": "This work introduces the first end-to-end coreference resolution model, trained<br>to maximize the marginal likelihood of gold antecedent spans from<br>coreference clusters and is factored to enable aggressive pruning of<br>potential mentions.",
        "170": "A new deep learning model for semantic role labeling (SRL)<br>that significantly improves the state of the art, along with<br>detailed analyses to reveal its strengths and limitations are introduced.",
        "171": "AllenNLP is designed to support researchers who want to build<br>novel language understanding models quickly and easily, and provides a<br>modular and extensible experiment framework that makes doing good science<br>easy.",
        "172": "This work presents a novel training procedure that can lift<br>the limitation of the relatively limited amount of labeled data<br>and the non-sequential nature of the AMR graphs, and presents<br>strong evidence that sequence-based AMR models are robust against ordering<br>variations of graph-to-sequence conversions.",
        "173": "Recurrent additive networks are introduced, a new gated RNN which<br>is distinguished by the use of purely additive latent state<br>updates, and it is formally shown that RAN states are<br>weighted sums of the input vectors, and that the gates<br>only contribute to computing the weights of these sums.",
        "174": "It is shown that, in comparison to other recently introduced<br>large-scale datasets, TriviaQA has relatively complex, compositional questions, has considerable<br>syntactic and lexical variability between questions and corresponding answer-evidence sentences,<br>and requires more cross sentence reasoning to find answers.",
        "175": "It is shown that relation extraction can be reduced to<br>answering simple reading comprehension questions, by associating one or more<br>natural-language questions with each relation slot, and that zero-shot generalization<br>to unseen relation types is possible, at lower accuracy levels.",
        "176": "This paper presents the first completely datadriven approach for generating<br>high level summaries of source code, which uses Long Short<br>Term Memory (LSTM) networks with attention to produce sentences that<br>describe C# code snippets and SQL queries.",
        "177": "This progress report summarizes the final findings and describes completed<br>work towards afull, interactive grounded learning system that can interpret<br>rich human input such as speech, gesture, body motion, and<br>gaze and learn to identify objects based on thenames and<br>attributes people use to refer to them.",
        "178": "The neural checklist model is presented, a recurrent neural network<br>that models global coherence by storing and updating an agenda<br>of text strings which should be mentioned somewhere in the<br>output by dynamically adjusting the interpolation among a language model<br>and a pair of attention models.",
        "179": "This paper studies semantic sparsity in situation recognition, the task<br>of producing structured summaries of what is happening in images,<br>including activities, objects and the roles objects play within the<br>activity.",
        "180": "This work demonstrates that a state-of-the-art parser can be built<br>using only a lexical tagging model and a deterministic grammar,<br>with no explicit model of bi-lexical dependencies, and can recover<br>long-range dependencies with high accuracy.",
        "181": "This paper presents a text generation method called It rewriting,<br>which edits existing human-authored narratives to change their theme without<br>changing the underlying story, and applies it to math word<br>problems, where it might help students stay more engaged by<br>quickly transforming all of their homework assignments to the theme<br>of their favorite movie without changes the math concepts that<br>are being taught.",
        "182": "This work introduces the first global recursive neural parsing model<br>with optimality guarantees during decoding, and shows it is possible<br>to learn an efficient A* parser.",
        "183": "",
        "184": "An ILP is introduced that jointly models sentence-and discourse-level sentiment<br>cues, factual evidence about entity factions, and global constraints based<br>on social science theories such as homophily, social balance, and<br>reciprocity, which allow for rich inference across groups of entities.",
        "185": "This paper demonstrates that it is possible for a parser<br>to improve its performance with a human in the loop,<br>by posing simple questions to non-experts, and applies the approach<br>to a CCG parser, converting uncertain attachment decisions into natural<br>language questions about the arguments of verbs.",
        "186": "It is found that non-experts, with very little training, can<br>reliably provide judgments about what events are mentioned and the<br>extent to which the author thinks they actually happened.",
        "187": "This work proposes a novel technique for automatic generation of<br>personalized word problems that takes a logical encoding of the<br>specification, synthesizes a word problem narrative and its mathematical model<br>as a labeled logical plot graph, and realizes the problem<br>in natural language.",
        "188": "A joint model using CCG is introduced, which is motivated<br>by the close link between CCG syntax and semantics and<br>is the first to substantially improve both syntactic and semantic<br>accuracy over a comparable pipeline, and also achieves state-of-the-art results<br>for a nonensemble semantic role labelling model.",
        "189": "The results show that non-expert annotators can produce high quality<br>QA-SRL data, and also establish baseline performance levels for future<br>work on this task, and introduce simple classifierbased models for<br>predicting which questions to ask and what their answers should<br>be.",
        "190": "A new semantic parsing approach is introduced that learns to<br>resolve such ontological mismatches with Freebase and uses a probabilistic<br>CCG to build linguistically motivated logicalform meaning representations, and includes<br>an ontology matching model that adapts the output logical forms<br>for each target ontology.",
        "191": "A natural language based interface for PbD that removes requirements<br>and enables hands-free programming and takes a natural language command<br>and the current world state to infer the intended movement<br>command and its parametrization.",
        "192": "A new model is presented that combines CCG parsing to<br>recover compositional aspects of meaning and a factor graph to<br>model non-compositional phenomena, such as anaphoric dependencies, which is significantly<br>outperforming the previous state of the art.",
        "193": "A novel system is presented, InstaRead, that streamlines authoring with<br>an ensemble of methods: encoding extraction rules in an expressive<br>and compositional representation, guiding the user to promising rules based<br>on corpus statistics and mined resources, and introducing a new<br>interactive development cycle that provides immediate feedback --- even on<br>large datasets.",
        "194": "An unsupervised hard EM approach to automatically mapping instructional recipes<br>to action graphs, which define what actions should be performed<br>on which objects and in what order, which incorporates aspects<br>of procedural semantics and world knowledge.",
        "195": "A new semantic parsing model and semi-supervised learning approach for<br>reasoning with partial ontological support is presented, which allows us<br>to improve precision over strong baselines, while parsing many phrases<br>that would be ignored by existing techniques.",
        "196": "A key ambition of AI is to render computers able<br>to evolve and interact with the real world if the<br>machine is able to produce an interpretation of its avail-able<br>modalities which can be used to support reasoning and takingappropriate<br>actions.",
        "197": "This work presents a system where a robot is interactively<br>trained by a user, grounding the robot\u2019s multimodal continuous sensor<br>data in natural language symbols, and allows the robot to<br>reason not only about the learned concepts, but the spaces<br>in-between them.",
        "198": "An approach for automatically learning to solve algebra word problems<br>by reasons across sentence boundaries to construct and solve a<br>system of linear equations, while simultaneously recovering an alignment of<br>the variables and numbers to the problem text.",
        "199": "This work investigates how people refer to objects in the<br>world during relatively unstructured communication with robots, and collects a<br>corpus of deictic interactions from users describing objects to train<br>language and gesture models that allow a robot to determine<br>what objects are being indicated.",
        "200": "This work uses a Combinatory Categorial Grammar to construct compositional<br>meaning representations, while considering contextual cues, such as the document<br>creation time and the tense of the governing verb, to<br>compute the final time values.",
        "201": "This paper collects human annotations of objects, parts, attributes and<br>activities in images to build a significantly more comprehensive model<br>of language generation and to study what visual information is<br>required to generate human-like descriptions.",
        "202": "It is demonstrated that significant performance gains can be achieved<br>in CCG semantic parsing by introducing a linguistically motivated grammar<br>induction scheme, and a new morpho-syntactic factored lexicon is presented<br>that models systematic variations in morphology, syntax, and semantics across<br>word classes.",
        "203": "",
        "204": "This paper presents OQA, the first approach to leverage both<br>curated and extracted KBs, and demonstrates that it achieves up<br>to twice the precision and recall of a state-of-the-art Open<br>QA system.",
        "205": "A posterior regularization technique is introduced which enforces soft constraints<br>on the classifiers, regularizing them to prefer sparse and low-rank<br>predictions, which results in classifiers which better fit the real<br>data.",
        "206": "The University of Washington Semantic Parsing Framework is a learning<br>and inference framework for mapping natural language to formal representation<br>of its meaning.",
        "207": "A unified approach for learning Combinatory Categorial Grammar (CCG) semantic<br>parsers, that induces both a CCG lexicon and the parameters<br>of a parsing model is described.",
        "208": "A new latent-variable approach that models missing data provides a<br>natural way to incorporate side information, for instance modeling the<br>intuition that text will often mention rare entities which are<br>likely to be missing in the database.",
        "209": "A novel approach for improving communication success between users of<br>speech-to-speech translation systems by automatically detecting errors in the output<br>of automatic speech recognition (ASR) and statistical machine translation (SMT)<br>systems.",
        "210": "This paper studies an example of a scenario in which<br>a visually impaired person and a robotic \u201cguide\u201d collaborate in<br>an unfamiliar environment, and analyzes how the scenario can be<br>realized through language- and gesture-based human-robot interaction, combined with semantic<br>spatial understanding and reasoning.",
        "211": "A novel approach is described that first automatically extracts task<br>knowledge from instructions, then learns a dialog manager over this<br>task knowledge to provide assistance, and can be integrated into<br>a dialog system that provides effective help to users.",
        "212": "An idiom classifier is trained on a newly gathered corpus<br>of over 60,000 Wiktionary multi-word definitions, incorporating features that model<br>whether phrase meanings are constructed compositionally.",
        "213": "It is demonstrated that combining multiple communication modalities is more<br>effective for understanding user intent than focusing on only one<br>type of input.",
        "214": "A new semantic parsing approach that learns to resolve ontological<br>mismatches, which is learned from question-answer pairs, uses a probabilistic<br>CCG to build linguistically motivated logicalform meaning representations, and includes<br>an ontology matching model that adapts the output logical forms<br>for each target ontology.",
        "215": "This work introduces an approach for analyzing Wikipedia and other<br>text, together with online photos, to produce annotated 3D models<br>of famous tourist sites, which leverages online text and photo<br>co-occurrences via Google Image Search.",
        "216": "This work demonstrates that it is possible to learn a<br>semantic lexicon and linear ranking function without manually annotating questions<br>and automatically generalizes a seed lexicon, and includes a scalable,<br>parallelized perceptron parameter estimation scheme.",
        "217": "This paper presents a new dataset, collected to describe Xbox<br>avatars, as well as models for learning the relationships between<br>these avatars and their literal and sentimental descriptions, and demonstrates<br>that sentimental language provides a concise (though noisy) means of<br>specifying low-level visual properties.",
        "218": "Despite an extremely large space of possible expressions, effective learning<br>of a globally normalized log-linear distribution is demonstrated by a<br>new, multi-stage approximate inference technique that uses a pruning model<br>to construct only the most likely logical forms.",
        "219": "NECO is introduced, a new model for named entity linking<br>and coreference resolution, which solves both problems jointly, reducing the<br>errors made on each.",
        "220": "This paper shows semantic parsing can be used within a<br>grounded CCG semantic parsing approach that learns a joint model<br>of meaning and context for interpreting and executing natural language<br>instructions, using various types of weak supervision.",
        "221": "This course will teach the fundamental ideas used in key<br>NLP components, organized into four parts: Probabilistic language models, which<br>define probability distributions over text passages, Analyzers, which map texts<br>into linguistic representations that enable various kinds of understanding, and<br>Generators, which produce natural language as output.",
        "222": "This work proposes a method which learns STRIPS action models<br>in such domains, by decomposing the problem into first learning<br>a transition function between states in the form of a<br>set of classifiers, and then deriving explicitSTRIPS rules from the<br>classifiers' parameters.",
        "223": "This work discusses the problem of parsing natural language commands<br>to actions and control structures that can be readily implemented<br>in a robot execution system, and learns a parser based<br>on example pairs of English commands and corresponding control language<br>expressions.",
        "224": "This dissertation shows that the amount of human effort necessary<br>to create relation extractors can be greatly reduced by leveraging<br>a richer set of user interactions, some of which use<br>more accurate models of weak supervision from a database.",
        "225": "This work presents an approach for joint learning of language<br>and perception models for grounded attribute induction, which includes a<br>language model based on a probabilistic categorial grammar that enables<br>the construction of compositional meaning representations.",
        "226": "A joint model for template filling, where the goal is<br>to automatically specify the fields of target relations such as<br>seminar announcements or corporate acquisition events, is presented, which demonstrates<br>effective learning with a Perceptron-style approach that uses simple, greedy<br>beam decoding.",
        "227": "This paper addresses the problem of detecting words that are<br>out-of-vocabulary for a speech recognition system to improve automatic speech<br>translation and achieves OOV detection F-scores of 60-66 and reduces<br>word error rate by 12% relative to the case where<br>OOV words are not detected.",
        "228": "RevMiner is introduced - a novel smartphone interface that utilizes<br>Natural Language Processing techniques to analyze and navigate reviews and<br>it is demonstrated that on a smartphone, participants preferred RevMiner's<br>interface to tag clouds and color bars, and that they<br>preferred Revminer's results to Yelp's, particularly for conjunctive queries.",
        "229": "An incremental probabilistic learner that models the acquistion of syntax<br>and semantics from a corpus of child-directed utterances paired with<br>possible representations of their meanings while also countering previous criticisms<br>of statistical syntactic learners is presented.",
        "230": "This paper aims to learn a parser mapping natural language<br>indoor route instructions into a LISP-like control language, and test<br>the approach using a simulator executing RCL programs on sets<br>of natural language route instructions given by non-experts.",
        "231": "This book describes a series of new techniques for learning<br>to map sentences to logical form --- lambda-calculus representations of<br>their meanings.",
        "232": "It is argued that one approach to dealing with this<br>type of humanrobot interaction is teachable robotics, in which robots<br>learn to perform novel tasks in novel environments from humans<br>using intuitive teaching modalities, such as natural language.",
        "233": "An algorithm for learning factored CCG lexicons, along with a<br>probabilistic parse-selection model, which includes both lexemes to model word<br>meaning and templates to model systematic variation in word usage<br>are presented.",
        "234": "This paper introduces a loss function to measure how well<br>potential meanings match the conversation, and induces a weighted CCG<br>grammar that could be used to automatically bootstrap the semantic<br>analysis component in a complete dialog system.",
        "235": "A novel approach for multi-instance learning with overlapping relations that<br>combines a sentence-level extraction model with a simple, corpus-level component<br>for aggregating the individual facts is presented.",
        "236": "A method that fills in missing information using an automatically<br>derived environment model that encodes states, transitions, and commands that<br>cause these transitions to happen that enables learning for mapping<br>high-level instructions, which previous statistical methods cannot handle.",
        "237": "A method that fills in missing information using an automatically<br>derived environment model that encodes states, transitions, and commands that<br>cause these transitions to happen that enables learning for mapping<br>high-level instructions, which previous statistical methods cannot handle.",
        "238": "This paper uses higher-order unification to define a hypothesis space<br>containing all grammars consistent with the training data, and develops<br>an online learning algorithm that efficiently searches this space while<br>simultaneously estimating the parameters of a log-linear parsing model.",
        "239": "An algorithm is developed that maintains explicit, lambda-calculus representations of<br>salient discourse entities and uses a context-dependent analysis pipeline to<br>recover logical forms and a hidden-variable variant of the perception<br>algorithm to learn a linear model used to select the<br>best analysis.",
        "240": "This paper presents a reinforcement learning approach for mapping natural<br>language instructions to sequences of executable actions, and uses a<br>policy gradient algorithm to estimate the parameters of a log-linear<br>model for action selection.",
        "241": "This paper formally defines an infinite sequence of nested beliefs<br>about the state of the world at the current time<br>t, and presents a filtering algorithm that maintains a finite<br>representation which can be used to generate these beliefs.",
        "242": "The generative model is applied to the task of mapping<br>sentences to hierarchical representations of their underlying meaning and achieves<br>state-of-the-art performance when tested on two publicly available corpora.",
        "243": "A key idea is to introduce non-standard CCG combinators that<br>relax certain parts of the grammar\u2014for example allowing flexible word<br>order, or insertion of lexical items\u2014 with learned costs.",
        "244": "A concrete problem in the context of planning meetings is<br>used to show how lifted probabilistic inference can dramatically speed<br>up reasoning, and lifted inference algorithms such as first-order VE<br>(FOVE) are extended to deal with cardinality potentials.",
        "245": "A hierarchical Bayesian approach is taken, in which the system<br>learns a prior distribution over rule sets, and a class<br>of prior distributions parameterized by a rule set prototype that<br>is stochastically modified to produce a task-specific rule set.",
        "246": "This paper presents a compact representation for such models and<br>an associated logical particle filtering algorithm that contains a logical<br>formula that describes a set of states, and demonstrates how<br>this filter can be more accurate than a traditional particle<br>filter in high dimensional state spaces.",
        "247": "This paper presents a technique for selecting the phrase pairs<br>to include in phrase translation tables based on their estimated<br>quality according to a translation model that not only reduces<br>the size of the phrase translation table, but also improves<br>translation quality as measured by the BLEU metric.",
        "248": "A probabilistic, relational planning rule representation is developed that compactly<br>models noisy, nondeterministic action effects, and it is demonstrated that<br>this learning algorithm allows agents to effectively model world dynamics.",
        "249": "A learning algorithm is described that takes as input a<br>training set of sentences labeled with expressions in the lambda<br>calculus and induces a grammar for the problem, along with<br>a log-linear model that represents a distribution over syntactic and<br>semantic analyses conditioned on the input sentence.",
        "250": "This work considers learning in a 3D simulated blocks world<br>with realistic physics, and presents an algorithm for learning a<br>model of the effects of actions in noisy stochastic worlds<br>that can create rules while also learning derived predicates.",
        "251": "A three-dimensional blocks-world simulation built with the OpenDynamics toolkit that<br>represents world action dynamics using probabilistic planning rules to take<br>advantage of the inherent structure found in many uncertain, complex<br>environments.",
        "252": "An algorithm for learning probabilistic STRIPS-like planning operators from examples<br>is presented and the effective learning of rule-based operators for<br>a wide range of traditional planning domains is demonstrated.",
        "253": "This paper describes the relationship between interface agents and the<br>theoretical and heuristic properties of user interfaces and develops a<br>novel type of interface agent, called an ibot, to exploit<br>these correspondences.",
        "254": "This work has developed a novel class of agents the<br>authors call interface softbots, or ibots, that control interactive applications<br>through the graphical user interface, as human users do, and<br>provides a general-purpose means of managing interactive applications, through the<br>same medium as a real user.",
        "255": "This chapter explores an approach using visual properties of the<br>interaction elements themselves, such as size, shape, color, and appearance<br>of graphical objects to describe user intentions to remove one<br>of the worst obstacles preventing the use of PBE with<br>commercial applications.",
        "256": "This work describes a three-phase avatar-based misconception correction framework in<br>which as students navigate their avatars through 3D learning environments,<br>a misconception detector tracks their problem-solving activities by inspecting task<br>networks, and the avatars employ a misconception corrector to intervene<br>with tailored advice.",
        "257": "",
        "258": "The flick gesture is described, designed for this purpose, and<br>experimental results demonstrate that the gesture performs well with regard<br>to speed, accuracy, and variability, compared to conventional gestures in<br>a laboratory setting.",
        "259": "A new class of animated pedagogical agents, explanatory lifelike avatars,<br>which can perform user-designed tasks in rich 3D worlds and<br>accompany them with running verbal explanations in realtime are described.",
        "260": "Two applications are discussed, a user-controlled visualscripting program and an<br>autonomous solitaire-playing program, which together demonstrate some of the capabilities<br>and limitation of the VisMap API-independent control approach.",
        "261": "Pilot studies suggest that habitable learning environments offer a promising<br>new paradigm for educational applications, and the Situated Avatar-Based Immersive<br>Learning (SAIL) framework for habitable 3D learning environments is used<br>to implement CPU CITY, a3D learning environment testbed for the<br>domain of computer architecture.",
        "262": "The cinematic task modeling framework for automated realtime task-sensitive camera<br>control in 3D environments is developed and a fullscale cinematography<br>interface and a 3D learning environment testbed are constructed.",
        "263": "The architecture of the ibot, a specialized software agent that<br>exists in the environment of the user interface, and its<br>algorithms for image processing, event management, and state representation are<br>described."
    },
    "venue": {
        "0": "arXiv.org",
        "1": "arXiv.org",
        "2": "arXiv.org",
        "3": "arXiv.org",
        "4": "arXiv.org",
        "5": "",
        "6": "arXiv.org",
        "7": "arXiv.org",
        "8": "arXiv.org",
        "9": "International Conference on Machine Learning",
        "10": "arXiv.org",
        "11": "arXiv.org",
        "12": "arXiv.org",
        "13": "arXiv.org",
        "14": "arXiv.org",
        "15": "arXiv.org",
        "16": "arXiv.org",
        "17": "arXiv.org",
        "18": "arXiv.org",
        "19": "arXiv.org",
        "20": "arXiv.org",
        "21": "International Conference on Machine Learning",
        "22": "arXiv.org",
        "23": "arXiv.org",
        "24": "arXiv.org",
        "25": "arXiv.org",
        "26": "arXiv.org",
        "27": "Annual Meeting of the Association for Computational Linguistics",
        "28": "Annual Meeting of the Association for Computational Linguistics",
        "29": "Conference on Empirical Methods in Natural Language Processing",
        "30": "Annual Meeting of the Association for Computational Linguistics",
        "31": "Conference on Empirical Methods in Natural Language Processing",
        "32": "arXiv.org",
        "33": "Annual Meeting of the Association for Computational Linguistics",
        "34": "arXiv.org",
        "35": "Neural Information Processing Systems",
        "36": "Conference on Empirical Methods in Natural Language Processing",
        "37": "Conference on Empirical Methods in Natural Language Processing",
        "38": "Conference on Empirical Methods in Natural Language Processing",
        "39": "arXiv.org",
        "40": "Annual Meeting of the Association for Computational Linguistics",
        "41": "International Conference on Learning Representations",
        "42": "International Conference on Machine Learning",
        "43": "Conference on Empirical Methods in Natural Language Processing",
        "44": "Conference on Empirical Methods in Natural Language Processing",
        "45": "IEEE/ACM Transactions on Audio Speech and Language Processing",
        "46": "Annual Meeting of the Association for Computational Linguistics",
        "47": "Conference on Empirical Methods in Natural Language Processing",
        "48": "Annual Meeting of the Association for Computational Linguistics",
        "49": "International Conference on Topology, Algebra and Categories in Logic",
        "50": "Annual Meeting of the Association for Computational Linguistics",
        "51": "International Conference on Learning Representations",
        "52": "Conference on Empirical Methods in Natural Language Processing",
        "53": "Conference on Empirical Methods in Natural Language Processing",
        "54": "arXiv.org",
        "55": "Conference on Empirical Methods in Natural Language Processing",
        "56": "International Conference on Machine Learning",
        "57": "Neural Information Processing Systems",
        "58": "arXiv.org",
        "59": "International Conference on Learning Representations",
        "60": "Conference on Empirical Methods in Natural Language Processing",
        "61": "AAAI Conference on Artificial Intelligence",
        "62": "Spoken Language Technology Workshop",
        "63": "Annual Meeting of the Association for Computational Linguistics",
        "64": "Annual Meeting of the Association for Computational Linguistics",
        "65": "arXiv.org",
        "66": "arXiv.org",
        "67": "arXiv.org",
        "68": "",
        "69": "arXiv.org",
        "70": "arXiv.org",
        "71": "arXiv.org",
        "72": "Conference of the European Chapter of the Association for Computational Linguistics",
        "73": "International Conference on Learning Representations",
        "74": "arXiv.org",
        "75": "Annual Meeting of the Association for Computational Linguistics",
        "76": "Findings",
        "77": "Annual Meeting of the Association for Computational Linguistics",
        "78": "Conference on Robot Learning",
        "79": "International Conference on Learning Representations",
        "80": "North American Chapter of the Association for Computational Linguistics",
        "81": "Conference on Empirical Methods in Natural Language Processing",
        "82": "North American Chapter of the Association for Computational Linguistics",
        "83": "arXiv.org",
        "84": "Findings",
        "85": "Findings",
        "86": "Conference on Empirical Methods in Natural Language Processing",
        "87": "North American Chapter of the Association for Computational Linguistics",
        "88": "Conference on Empirical Methods in Natural Language Processing",
        "89": "Neural Information Processing Systems",
        "90": "International Conference on Machine Learning",
        "91": "NAACL-HLT",
        "92": "Annual Meeting of the Association for Computational Linguistics",
        "93": "Findings",
        "94": "International Conference on Learning Representations",
        "95": "Annual Meeting of the Association for Computational Linguistics",
        "96": "Transactions of the Association for Computational Linguistics",
        "97": "Conference on Empirical Methods in Natural Language Processing",
        "98": "Findings",
        "99": "International Conference on Computational Linguistics",
        "100": "International Conference on Learning Representations",
        "101": "Conference on Empirical Methods in Natural Language Processing",
        "102": "Annual Meeting of the Association for Computational Linguistics",
        "103": "International Conference on Machine Learning",
        "104": "Annual Meeting of the Association for Computational Linguistics",
        "105": "Transactions of the Association for Computational Linguistics",
        "106": "Findings",
        "107": "arXiv.org",
        "108": "Annual Meeting of the Association for Computational Linguistics",
        "109": "arXiv.org",
        "110": "International Conference on Learning Representations",
        "111": "Neural Information Processing Systems",
        "112": "Conference on Empirical Methods in Natural Language Processing",
        "113": "Conference on Empirical Methods in Natural Language Processing",
        "114": "Annual Meeting of the Association for Computational Linguistics",
        "115": "Conference on Empirical Methods in Natural Language Processing",
        "116": "Annual Meeting of the Association for Computational Linguistics",
        "117": "arXiv.org",
        "118": "Conference on Empirical Methods in Natural Language Processing",
        "119": "Annual Meeting of the Association for Computational Linguistics",
        "120": "arXiv.org",
        "121": "International Joint Conference on Natural Language Processing",
        "122": "Conference on Robot Learning",
        "123": "Annual Meeting of the Association for Computational Linguistics",
        "124": "Annual Meeting of the Association for Computational Linguistics",
        "125": "Conference on Empirical Methods in Natural Language Processing",
        "126": "arXiv.org",
        "127": "Annual Meeting of the Association for Computational Linguistics",
        "128": "arXiv.org",
        "129": "Conference on Empirical Methods in Natural Language Processing",
        "130": "arXiv.org",
        "131": "arXiv.org",
        "132": "Conference on Empirical Methods in Natural Language Processing",
        "133": "Annual Meeting of the Association for Computational Linguistics",
        "134": "Conference on Empirical Methods in Natural Language Processing",
        "135": "Annual Meeting of the Association for Computational Linguistics",
        "136": "International Conference on Learning Representations",
        "137": "Transactions of the Association for Computational Linguistics",
        "138": "Annual Meeting of the Association for Computational Linguistics",
        "139": "Annual Meeting of the Association for Computational Linguistics",
        "140": "Conference on Empirical Methods in Natural Language Processing",
        "141": "arXiv.org",
        "142": "North American Chapter of the Association for Computational Linguistics",
        "143": "arXiv.org",
        "144": "Annual Meeting of the Association for Computational Linguistics",
        "145": "Conference on Empirical Methods in Natural Language Processing",
        "146": "Computer Vision and Pattern Recognition",
        "147": "Conference on Empirical Methods in Natural Language Processing",
        "148": "Conference on Empirical Methods in Natural Language Processing",
        "149": "North American Chapter of the Association for Computational Linguistics",
        "150": "Annual Meeting of the Association for Computational Linguistics",
        "151": "Conference on Empirical Methods in Natural Language Processing",
        "152": "Annual Meeting of the Association for Computational Linguistics",
        "153": "North American Chapter of the Association for Computational Linguistics",
        "154": "Annual Meeting of the Association for Computational Linguistics",
        "155": "International Conference on Language Resources and Evaluation",
        "156": "Annual Meeting of the Association for Computational Linguistics",
        "157": "North American Chapter of the Association for Computational Linguistics",
        "158": "North American Chapter of the Association for Computational Linguistics",
        "159": "Annual Meeting of the Association for Computational Linguistics",
        "160": "arXiv.org",
        "161": "Conference on Empirical Methods in Natural Language Processing",
        "162": "Conference on Empirical Methods in Natural Language Processing",
        "163": "Conference on Empirical Methods in Natural Language Processing",
        "164": "Proceedings on Privacy Enhancing Technologies",
        "165": "North American Chapter of the Association for Computational Linguistics",
        "166": "Annual Meeting of the Association for Computational Linguistics",
        "167": "North American Chapter of the Association for Computational Linguistics",
        "168": "Annual Meeting of the Association for Computational Linguistics",
        "169": "Conference on Empirical Methods in Natural Language Processing",
        "170": "Annual Meeting of the Association for Computational Linguistics",
        "171": "",
        "172": "Annual Meeting of the Association for Computational Linguistics",
        "173": "arXiv.org",
        "174": "Annual Meeting of the Association for Computational Linguistics",
        "175": "Conference on Computational Natural Language Learning",
        "176": "Annual Meeting of the Association for Computational Linguistics",
        "177": "",
        "178": "Conference on Empirical Methods in Natural Language Processing",
        "179": "Computer Vision and Pattern Recognition",
        "180": "North American Chapter of the Association for Computational Linguistics",
        "181": "Conference on Empirical Methods in Natural Language Processing",
        "182": "Conference on Empirical Methods in Natural Language Processing",
        "183": "Computer Vision and Pattern Recognition",
        "184": "Annual Meeting of the Association for Computational Linguistics",
        "185": "Conference on Empirical Methods in Natural Language Processing",
        "186": "Conference on Empirical Methods in Natural Language Processing",
        "187": "International Joint Conference on Artificial Intelligence",
        "188": "Conference on Empirical Methods in Natural Language Processing",
        "189": "Conference on Empirical Methods in Natural Language Processing",
        "190": "",
        "191": "IEEE International Conference on Robotics and Automation",
        "192": "Conference on Empirical Methods in Natural Language Processing",
        "193": "arXiv.org",
        "194": "Conference on Empirical Methods in Natural Language Processing",
        "195": "Annual Meeting of the Association for Computational Linguistics",
        "196": "Machine-mediated learning",
        "197": "",
        "198": "Annual Meeting of the Association for Computational Linguistics",
        "199": "AAAI Conference on Artificial Intelligence",
        "200": "Annual Meeting of the Association for Computational Linguistics",
        "201": "International Workshop on Semantic Evaluation",
        "202": "Conference on Empirical Methods in Natural Language Processing",
        "203": "",
        "204": "Knowledge Discovery and Data Mining",
        "205": "",
        "206": "arXiv.org",
        "207": "Annual Meeting of the Association for Computational Linguistics",
        "208": "Transactions of the Association for Computational Linguistics",
        "209": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "210": "AAAI Conference on Artificial Intelligence",
        "211": "Annual Meeting of the Association for Computational Linguistics",
        "212": "Conference on Empirical Methods in Natural Language Processing",
        "213": "",
        "214": "Conference on Empirical Methods in Natural Language Processing",
        "215": "ACM Transactions on Graphics",
        "216": "Annual Meeting of the Association for Computational Linguistics",
        "217": "North American Chapter of the Association for Computational Linguistics",
        "218": "Conference on Empirical Methods in Natural Language Processing",
        "219": "Conference on Empirical Methods in Natural Language Processing",
        "220": "Transactions of the Association for Computational Linguistics",
        "221": "",
        "222": "Conference on Uncertainty in Artificial Intelligence",
        "223": "International Symposium on Experimental Robotics",
        "224": "",
        "225": "International Conference on Machine Learning",
        "226": "Annual Meeting of the Association for Computational Linguistics",
        "227": "Spoken Language Technology Workshop",
        "228": "ACM Symposium on User Interface Software and Technology",
        "229": "Conference of the European Chapter of the Association for Computational Linguistics",
        "230": "",
        "231": "Conference on Uncertainty in Artificial Intelligence",
        "232": "",
        "233": "Conference on Empirical Methods in Natural Language Processing",
        "234": "Conference on Empirical Methods in Natural Language Processing",
        "235": "Annual Meeting of the Association for Computational Linguistics",
        "236": "Annual Meeting of the Association for Computational Linguistics",
        "237": "",
        "238": "Conference on Empirical Methods in Natural Language Processing",
        "239": "Annual Meeting of the Association for Computational Linguistics",
        "240": "Annual Meeting of the Association for Computational Linguistics",
        "241": "Neural Information Processing Systems",
        "242": "Conference on Empirical Methods in Natural Language Processing",
        "243": "Conference on Empirical Methods in Natural Language Processing",
        "244": "NIPS 2007",
        "245": "Conference on Uncertainty in Artificial Intelligence",
        "246": "Probabilistic, Logical and Relational Learning - A Further Synthesis",
        "247": "North American Chapter of the Association for Computational Linguistics",
        "248": "Journal of Artificial Intelligence Research",
        "249": "Conference on Uncertainty in Artificial Intelligence",
        "250": "AAAI Conference on Artificial Intelligence",
        "251": "",
        "252": "International Conference on Automated Planning and Scheduling",
        "253": "International Conference on Autonomous Agents",
        "254": "AAAI/IAAI",
        "255": "CACM",
        "256": "",
        "257": "",
        "258": "IFIP TC13 International Conference on Human-Computer Interaction",
        "259": "International Conference on Autonomous Agents",
        "260": "International Conference on Human Factors in Computing Systems",
        "261": "International Conference on Intelligent Tutoring Systems",
        "262": "International Conference on Intelligent User Interfaces",
        "263": "International Conference on Intelligent User Interfaces"
    }
}