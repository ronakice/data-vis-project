{
    "x": {
        "0": 78.9731216430664,
        "1": 146.7221221923828,
        "2": -8.22962760925293,
        "3": -39.414947509765625,
        "4": -96.59534454345703,
        "5": 8.323688507080078,
        "6": 56.83519744873047,
        "7": 32.370323181152344,
        "8": 91.67837524414062,
        "9": -128.07423400878906,
        "10": -89.67561340332031,
        "11": -51.6895866394043,
        "12": -18.03660011291504,
        "13": 110.53899383544922,
        "14": -93.7975845336914,
        "15": 140.32899475097656,
        "16": -141.91131591796875,
        "17": 23.818164825439453,
        "18": -3.735381841659546,
        "19": 71.59297943115234,
        "20": -62.39371871948242,
        "21": -166.69854736328125
    },
    "y": {
        "0": -54.07991027832031,
        "1": -83.19052124023438,
        "2": 139.12290954589844,
        "3": -106.89384460449219,
        "4": -154.1175994873047,
        "5": -46.07184600830078,
        "6": 9.579649925231934,
        "7": -109.28463745117188,
        "8": -150.40066528320312,
        "9": -79.92969512939453,
        "10": 135.8835906982422,
        "11": 71.06001281738281,
        "12": 12.313769340515137,
        "13": 65.84374237060547,
        "14": 12.807686805725098,
        "15": -1.8430023193359375,
        "16": 70.21977996826172,
        "17": 68.59463500976562,
        "18": -178.77137756347656,
        "19": 131.92062377929688,
        "20": -44.69953536987305,
        "21": -13.011953353881836
    },
    "title": {
        "0": "How Does Generative Retrieval Scale to Millions of Passages?",
        "1": "Zero-Shot Listwise Document Reranking with a Large Language Model",
        "2": "ReadProbe: A Demo of Retrieval-Enhanced Large Language Models to Support Lateral Reading",
        "3": "Vector Search with OpenAI Embeddings: Lucene Is All You Need",
        "4": "RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models",
        "5": "Neural Query Synthesis and Domain-Specific Ranking Templates for Multi-Stage Clinical Trial Matching",
        "6": "Document Expansion Baselines and Learned Sparse Lexical Representations for MS MARCO V1 and V2",
        "7": "Another Look at DPR: Reproduction of Training and Replication of Retrieval",
        "8": "Squeezing Water from a Stone: A Bag of Tricks for Further Improving Cross-Encoder Effectiveness for Reranking",
        "9": "Document Expansions and Learned Sparse Lexical Representations for MS MARCO V1 and V2",
        "10": "Chatty Goose: A Python Framework for Conversational Search",
        "11": "Exploring Listwise Evidence Reasoning with T5 for Fact Verification",
        "12": "Comparison of Foveated Downsampling Techniques in Image Recognition",
        "13": "Pyserini: An Easy-to-Use Python Toolkit to Support Replicable IR Research with Sparse and Dense Representations",
        "14": "Vera: Prediction Techniques for Reducing Harmful Misinformation in Consumer Health Search",
        "15": "A Replication Study of Dense Passage Retriever",
        "16": "Pyserini: A Python Toolkit for Reproducible Information Retrieval Research with Sparse and Dense Representations",
        "17": "The Expando-Mono-Duo Design Pattern for Text Ranking with Pretrained Sequence-to-Sequence Models",
        "18": "H2oloo at TREC 2020: When all you got is a hammer... Deep Learning, Health Misinformation, and Precision Medicine",
        "19": "Covidex: Neural Ranking Models and Keyword Search Infrastructure for the COVID-19 Open Research Dataset",
        "20": "Scientific Claim Verification with VerT5erini",
        "21": "Document Ranking with a Pretrained Sequence-to-Sequence Model"
    },
    "year": {
        "0": 2023,
        "1": 2023,
        "2": 2023,
        "3": 2023,
        "4": 2023,
        "5": 2022,
        "6": 2022,
        "7": 2022,
        "8": 2022,
        "9": 2022,
        "10": 2021,
        "11": 2021,
        "12": 2021,
        "13": 2021,
        "14": 2021,
        "15": 2021,
        "16": 2021,
        "17": 2021,
        "18": 2020,
        "19": 2020,
        "20": 2020,
        "21": 2020
    },
    "cluster": {
        "0": 3,
        "1": 0,
        "2": 3,
        "3": 3,
        "4": 0,
        "5": 0,
        "6": 3,
        "7": 3,
        "8": 3,
        "9": 3,
        "10": 1,
        "11": 2,
        "12": 4,
        "13": 1,
        "14": 2,
        "15": 3,
        "16": 1,
        "17": 0,
        "18": 2,
        "19": 1,
        "20": 2,
        "21": 0
    },
    "authors": {
        "0": "Ronak Pradeep, Kai Hui, Jai Gupta, \u00c1. Lelkes, Honglei Zhuang, et al.",
        "1": "Xueguang Ma, Xinyu Crystina Zhang, Ronak Pradeep, and Jimmy Lin",
        "2": "Dake Zhang and Ronak Pradeep",
        "3": "Jimmy Lin, Ronak Pradeep, Tommaso Teofili, and Jasper Xian",
        "4": "Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin",
        "5": "Ronak Pradeep, Yilin Li, Yuetong Wang, and Jimmy Lin",
        "6": "Xueguang Ma, Ronak Pradeep, Rodrigo Nogueira, and Jimmy Lin",
        "7": "Xueguang Ma, Kai Sun, Ronak Pradeep, Minghan Li, and Jimmy Lin",
        "8": "Ronak Pradeep, Yuqi Liu, Xinyu Crystina Zhang, Yilin Li, Andrew Yates, et al.",
        "9": "Ronak Pradeep, Rodrigo Nogueira, and Jimmy Lin",
        "10": "Edwin Zhang, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, Rodrigo Nogueira, et al.",
        "11": "Kelvin Jiang, Ronak Pradeep, and Jimmy Lin",
        "12": "Parsa Torabian, Ronak Pradeep, and Jeff Orchard",
        "13": "Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, et al.",
        "14": "Ronak Pradeep, Xueguang Ma, Rodrigo Nogueira, and Jimmy Lin",
        "15": "Xueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy Lin",
        "16": "Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, et al.",
        "17": "Ronak Pradeep, Rodrigo Nogueira, and Jimmy Lin",
        "18": "Ronak Pradeep, Xueguang Ma, Xinyu Crystina Zhang, H. Cui, Ruizhou Xu, et al.",
        "19": "Edwin Zhang, Nikhil Gupta, Raphael Tang, Xiao Han, Ronak Pradeep, et al.",
        "20": "Ronak Pradeep, Xueguang Ma, Rodrigo Nogueira, and Jimmy Lin",
        "21": "Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin"
    },
    "first_author": {
        "0": true,
        "1": false,
        "2": false,
        "3": false,
        "4": true,
        "5": true,
        "6": false,
        "7": false,
        "8": true,
        "9": true,
        "10": false,
        "11": false,
        "12": false,
        "13": false,
        "14": true,
        "15": false,
        "16": false,
        "17": true,
        "18": true,
        "19": false,
        "20": true,
        "21": false
    },
    "last_author": {
        "0": false,
        "1": false,
        "2": true,
        "3": false,
        "4": false,
        "5": false,
        "6": false,
        "7": false,
        "8": false,
        "9": false,
        "10": false,
        "11": false,
        "12": false,
        "13": false,
        "14": false,
        "15": false,
        "16": false,
        "17": false,
        "18": false,
        "19": false,
        "20": false,
        "21": false
    },
    "middle_author": {
        "0": false,
        "1": true,
        "2": false,
        "3": true,
        "4": false,
        "5": false,
        "6": true,
        "7": true,
        "8": false,
        "9": false,
        "10": true,
        "11": true,
        "12": true,
        "13": true,
        "14": false,
        "15": true,
        "16": true,
        "17": false,
        "18": false,
        "19": true,
        "20": false,
        "21": true
    },
    "author_of_interest": {
        "0": "Ronak Pradeep",
        "1": "Ronak Pradeep",
        "2": "Ronak Pradeep",
        "3": "Ronak Pradeep",
        "4": "Ronak Pradeep",
        "5": "Ronak Pradeep",
        "6": "Ronak Pradeep",
        "7": "Ronak Pradeep",
        "8": "Ronak Pradeep",
        "9": "Ronak Pradeep",
        "10": "Ronak Pradeep",
        "11": "Ronak Pradeep",
        "12": "Ronak Pradeep",
        "13": "Ronak Pradeep",
        "14": "Ronak Pradeep",
        "15": "Ronak Pradeep",
        "16": "Ronak Pradeep",
        "17": "Ronak Pradeep",
        "18": "Ronak Pradeep",
        "19": "Ronak Pradeep",
        "20": "Ronak Pradeep",
        "21": "Ronak Pradeep"
    },
    "reference_count": {
        "0": 45,
        "1": 28,
        "2": 21,
        "3": 29,
        "4": 41,
        "5": 14,
        "6": 30,
        "7": 19,
        "8": 53,
        "9": 29,
        "10": 24,
        "11": 34,
        "12": 12,
        "13": 31,
        "14": 25,
        "15": 14,
        "16": 41,
        "17": 64,
        "18": 18,
        "19": 44,
        "20": 34,
        "21": 38
    },
    "citation_count": {
        "0": 6,
        "1": 11,
        "2": 0,
        "3": 2,
        "4": 2,
        "5": 6,
        "6": 8,
        "7": 8,
        "8": 11,
        "9": 8,
        "10": 3,
        "11": 24,
        "12": 1,
        "13": 57,
        "14": 20,
        "15": 37,
        "16": 197,
        "17": 79,
        "18": 24,
        "19": 57,
        "20": 39,
        "21": 276
    },
    "tldr": {
        "0": "This work conducts the first empirical study of generative retrieval<br>techniques across various corpus scales, ultimately scaling up to the<br>entire MS MARCO passage ranking task with a corpus of<br>8.8M passages and evaluating model sizes up to 11B parameters.",
        "1": "This work proposes Listwise Reranker with a Large Language Model<br>(LRL), which achieves strong reranking effectiveness without using any task-specific<br>training data, and applies its approach to subsets of MIRACL,<br>a recent multilingual retrieval dataset, with results showing its potential<br>to generalize across different languages.",
        "2": "This paper presents ReadProbe, a tool to support lateral reading,<br>powered by generative large language models from OpenAI and the<br>Bing search engine, able to generate useful questions for lateral<br>read, scour the web for relevant documents, and generate well-attributed<br>answers to help people better evaluate online information.",
        "3": "It is shown that hierarchical navigable small-world network indexes in<br>Lucene are adequate to provide vector search capabilities in a<br>standard bi-encoder architecture.",
        "4": "RankVicuna is presented, the first fully open-source LLM capable of<br>performing high-quality listwise reranking in a zero-shot setting and it<br>is hoped this work provides the foundation for future research<br>on reranking with modern LLMs.",
        "5": "This work introduces NQS, a neural query synthesis method that<br>leverages a zero-shot document expansion model to generate multiple sentence-long<br>queries from lengthy patient descriptions and introduces a two-stage neural<br>reranking pipeline trained on clinical trial matching data using tailored<br>ranking templates.",
        "6": "A number of resources that support competitive, reproducible baselines for<br>both the MS MARCO V1 and V2 test collections using<br>the Anserini and Pyserini IR toolkits are described, providing a<br>solid foundation for future research on neural retrieval models using<br>the MSMARCO datasets and beyond.",
        "7": "By incorporating evidence from the retriever and improved answer span<br>scoring, this work manages to improve end-to-end question answering e\ufb00ectiveness<br>using the same DPR models.",
        "8": "This work presents a replication study of LCE on a<br>replication task and combines it with several other \u201ctricks\u201d to<br>substantially improve ranking e\ufb00ectiveness and attempts to more system-atically explore<br>certain parts of the hyperparameter space, including the choice of<br>losses and the group size in the LCE loss.",
        "9": "A number of resources that support competitive, reproducible baselines for<br>both the MS MARCO V1 and V2 test collections using<br>the Anserini and Pyserini IR toolkits are described, providing a<br>solid foundation for future research on neural retrieval models using<br>the MSMARCO datasets and beyond.",
        "10": "An overview of the Chatty Goose framework is provided and<br>how to instantiate a new system from scratch is demonstrated,<br>so that a comparable run can be reproduced with just<br>a few lines of code.",
        "11": "A framework for fact verification that leverages pretrained sequence-to-sequence transformer<br>models for sentence selection and label prediction, two key sub-tasks<br>in fact verification is explored, improving on previous pointwise aggregation<br>approaches for label prediction and taking advantage of T5 using<br>a listwise approach coupled with data augmentation.",
        "12": "Foveation does not substantially help or hinder object recognition in<br>deep networks, and the best variable-resolution method slightly outperforms uniform<br>downsampling.",
        "13": "An overview of toolkit features and empirical results that illustrate<br>its effectiveness on two popular ranking tasks are presented and<br>how the group has built a culture of replicability through<br>shared norms and tools that enable rigorous automated testing is<br>described.",
        "14": "This work proposes a label prediction technique that can separate<br>helpful from harmful content and leverages pretrained sequence-to-sequence transformer models<br>for both relevance ranking and label prediction.",
        "15": "By incorporating evidence from the retriever and an improved answer<br>span scoring technique, a replication study of the dense passage<br>retriever technique is able to improve end-to-end question answering effectiveness<br>using exactly the same models as in the original work.",
        "16": "An overview of toolkit features is provided and empirical results<br>that illustrate its effectiveness on two popular ranking tasks are<br>presented, as well as hybrid retrieval that integrates both approaches.",
        "17": "A design pattern for tackling text ranking problems, dubbed \"Expando-Mono-Duo\",<br>that has been empirically validated for a number of ad<br>hoc retrieval tasks in different domains, and implementations of the<br>design are open-sourced in the Pyserini IR toolkit and PyGaggle<br>neural reranking library.",
        "18": "The h2oloo team from the University of Waterloo participated in<br>the TREC 2020 Deep Learning, Health Misinformation, and Precision Medicine<br>Tracks, demonstrating the versatility and the zero-shot transfer capabilities of<br>the multi-stage ranking system.",
        "19": "Covidex, a search engine that exploits the latest neural ranking<br>models to provide information access to the COVID-19 Open Research<br>Dataset curated by the Allen Institute for AI, is presented.",
        "20": "The adaptation of a pretrained sequence-to-sequence model to the task<br>of scientific claim verification in the biomedical domain is described<br>and a system called VerT5erini is proposed that exploits T5<br>for abstract retrieval, sentence selection, and label prediction, which are<br>three critical sub-tasks of claim verification.",
        "21": "Surprisingly, it is found that the choice of target tokens<br>impacts effectiveness, even for words that are closely related semantically,<br>which sheds some light on why the sequence-to-sequence formulation for<br>document ranking is effective."
    },
    "venue": {
        "0": "arXiv.org",
        "1": "arXiv.org",
        "2": "arXiv.org",
        "3": "arXiv.org",
        "4": "",
        "5": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "6": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "7": "European Conference on Information Retrieval",
        "8": "European Conference on Information Retrieval",
        "9": "",
        "10": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "11": "Annual Meeting of the Association for Computational Linguistics",
        "12": "",
        "13": "arXiv.org",
        "14": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "15": "arXiv.org",
        "16": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "17": "arXiv.org",
        "18": "Text Retrieval Conference",
        "19": "SDP",
        "20": "International Workshop on Health Text Mining and Information Analysis",
        "21": "Findings"
    }
}